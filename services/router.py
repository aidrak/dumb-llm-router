import uuid
import time
import tiktoken
import json
from typing import Dict, Any, List
from fastapi import HTTPException

from clients.base import get_llm_client_and_model_details
from services.classifier import classifier
from config.models import ChatRequest, Message
from config.settings import settings
from utils.logger import setup_logger

logger = setup_logger(__name__)

class SmartRouter:
    def __init__(self):
        pass
    
    def _get_model_mappings(self) -> Dict[str, str]:
        return {
            "simple_no_research": settings.simple_no_research_model,
            "simple_research": settings.simple_research_model,
            "hard_no_research": settings.hard_no_research_model,
            "hard_research": settings.hard_research_model,
        }
    
    def _get_text_from_content(self, content: Any) -> str:
        if isinstance(content, str):
            return content
        if isinstance(content, list):
            return " ".join(part.get("text", "") for part in content if part.get("type") == "text")
        return ""

    def _detect_heavy_context(self, messages: List[Message]) -> bool:
        try:
            encoding = tiktoken.get_encoding("cl100k_base")
        except Exception as e:
            logger.warning(f"Could not get tiktoken encoding, falling back to character count: {e}")
            total_length = sum(len(self._get_text_from_content(msg.content)) for msg in messages)
            return total_length > (settings.token_usage_threshold * 4)

        num_tokens = 0
        for message in messages:
            text_content = self._get_text_from_content(message.content)
            if text_content:
                num_tokens += len(encoding.encode(text_content))
        return num_tokens > settings.token_usage_threshold

    def _has_image_in_current_message(self, message: Message) -> bool:
        """Check if the current message contains an image"""
        if isinstance(message.content, list):
            has_image = any(part.get("type") == "image_url" for part in message.content)
            if has_image:
                logger.info(f"üñºÔ∏è FOUND IMAGE in current message!")
            return has_image
        return False

    def _has_image_in_conversation(self, messages: List[Message]) -> bool:
        """Check if any message in the conversation contains an image"""
        for i, message in enumerate(messages):
            if self._has_image_in_current_message(message):
                logger.info(f"üñºÔ∏è FOUND IMAGE in message #{i+1}")
                return True
        return False

    def _references_image_content(self, text: str) -> bool:
        """Check if the text references image-related content"""
        image_reference_keywords = [
            "image", "picture", "photo", "screenshot", "device", "list", "show", "display",
            "read", "see", "view", "visible", "shown", "listed", "those", "these",
            "what's in", "what is in", "describe", "caption", "identify", "items",
            "objects", "contents", "details", "information", "from the image",
            "in the picture", "what you see", "analyze this", "tell me about",
            "third item", "first item", "second item", "last item", "bottom", "top",
            "left", "right", "corner", "center", "highlighted", "selected",
            "what's the", "which one", "how many", "count", "number of"
        ]
        
        text_lower = text.lower()
        matched_keywords = [kw for kw in image_reference_keywords if kw in text_lower]
        
        if matched_keywords:
            logger.info(f"üîç MATCHED IMAGE KEYWORDS: {matched_keywords}")
        
        return len(matched_keywords) > 0

    def _should_use_vision_model(self, messages: List[Message], conversation_state: Dict[str, Any]) -> bool:
        """Determine if we should use a vision-capable model"""
        last_message = messages[-1] if messages else None
        if not last_message:
            return False
        
        logger.info("üîç CHECKING VISION REQUIREMENTS...")
        
        # Check if current message has an image
        if self._has_image_in_current_message(last_message):
            logger.info("‚úÖ VISION REQUIRED: Current message contains image")
            return True
        
        # Check if conversation has images and current message references them
        if self._has_image_in_conversation(messages):
            last_text = self._get_text_from_content(last_message.content)
            logger.info(f"üîç CHECKING IMAGE REFERENCES in text: '{last_text}'")
            
            if self._references_image_content(last_text):
                logger.info("‚úÖ VISION REQUIRED: Text references image content")
                return True
            
            # If the message is short and potentially referring to something visual
            if len(last_text.strip()) < 50:
                logger.info("‚úÖ VISION REQUIRED: Short message with images in conversation")
                return True
        
        logger.info("‚ùå VISION NOT REQUIRED")
        return False

    def _determine_model_tier(self, category: str, needs_vision: bool, needs_heavy_context: bool) -> str:
        if needs_vision or needs_heavy_context:
            return "hard"
        if "hard" in category:
            return "hard"
        if "simple" in category:
            return "simple"
        return "simple"

    async def route_and_process(self, chat_request: ChatRequest, auth_header: str) -> Dict[str, Any]:
        settings.reload_if_changed()
        
        messages = chat_request.messages
        if not messages:
            raise HTTPException(status_code=400, detail="No messages provided.")

        last_message = messages[-1]
        user_prompt_text = self._get_text_from_content(last_message.content)
        
        logger.info(f"üöÄ ROUTING REQUEST: '{user_prompt_text[:100]}...'")
        logger.info(f"üìä CONVERSATION LENGTH: {len(messages)} messages")
        
        # Debug: Log message content types
        for i, msg in enumerate(messages):
            content_type = "string" if isinstance(msg.content, str) else f"list[{len(msg.content)}]"
            logger.info(f"üìù Message {i+1} ({msg.role}): {content_type}")
            if isinstance(msg.content, list):
                for j, part in enumerate(msg.content):
                    logger.info(f"   Part {j+1}: {part.get('type', 'unknown')}")
        
        # Get conversation state
        conversation_state = {} # This will be replaced by a call to the conversation state manager

        # Check for vision and heavy context requirements first
        needs_vision = self._should_use_vision_model(messages, conversation_state)
        needs_heavy_context = self._detect_heavy_context(messages)
        
        if needs_vision:
            logger.info("üñºÔ∏è Vision model required - routing to vision-capable model.")
            target_model = settings.hard_no_research_model
            model_tier = "hard"
            
        elif needs_heavy_context:
            logger.info("üìö Heavy context detected, routing to heavy model.")
            target_model = settings.hard_no_research_model
            model_tier = "hard"
            
        elif classifier.is_title_generation_request([msg.dict() for msg in messages]):
            logger.info("üè∑Ô∏è Title generation request detected, routing to simple model.")
            target_model = settings.simple_no_research_model
            model_tier = "simple"
            
        elif classifier.is_escalation_request(user_prompt_text):
            logger.info("‚¨ÜÔ∏è Escalation request detected, routing to escalation model.")
            target_model = settings.escalation_model
            model_tier = "escalation"
            
        elif classifier.is_research_request(user_prompt_text):
            logger.info("üîç Research request detected, routing to Perplexity.")
            target_model = "Perplexity-Research"
            model_tier = "research"
            
        else:
            # Normal classification handles simple_research and hard_research
            message_dicts = [msg.dict() for msg in messages]
            category = await classifier.classify_message(message_dicts)
            model_mappings = self._get_model_mappings()
            target_model = model_mappings.get(category, settings.fallback_model)
            model_tier = self._determine_model_tier(category, needs_vision, needs_heavy_context)
            
            logger.info(f"üéØ Classified as '{category}', routing to '{target_model}'")

        logger.info(f"üéØ TARGET MODEL SELECTED: {target_model}")

        client, model_id, model_type, api_params, system_prompt = get_llm_client_and_model_details(target_model)
        
        if not client or not model_id:
            logger.warning(f"‚ö†Ô∏è Target model '{target_model}' not available, trying fallback.")
            client, model_id, model_type, api_params, system_prompt = get_llm_client_and_model_details(settings.fallback_model)
            if not client or not model_id:
                raise HTTPException(status_code=500, detail="No valid LLM client available")
            target_model = settings.fallback_model

        # CRITICAL DEBUG INFO
        provider_name = getattr(client, 'provider_name', 'unknown')
        logger.info(f"üîß FINAL ROUTING DECISION:")
        logger.info(f"   üìã Logical Model: {target_model}")
        logger.info(f"   üè≠ Provider: {provider_name}")
        logger.info(f"   ü§ñ Actual Model ID: {model_id}")
        logger.info(f"   üñºÔ∏è Vision Required: {needs_vision}")
        
        # Double-check: If vision is required but we're not using Gemini, FORCE Gemini
        if needs_vision and provider_name != 'gemini':
            logger.error(f"üö® CRITICAL ERROR: Vision required but routing to {provider_name}!")
            logger.error(f"üîß FORCING GEMINI MODEL...")
            
            # Force to Gemini Pro
            client, model_id, model_type, api_params, system_prompt = get_llm_client_and_model_details(settings.escalation_model)
            if client and getattr(client, 'provider_name', '') == 'gemini':
                target_model = settings.escalation_model
                provider_name = 'gemini'
                logger.info(f"‚úÖ FORCED TO GEMINI: {target_model} -> {model_id}")
            else:
                raise HTTPException(status_code=500, detail="Cannot find Gemini model for vision task!")
        
        try:
            temperature = chat_request.temperature if chat_request.temperature is not None else 0.7
            message_dicts = [msg.dict() for msg in messages]
            
            logger.info(f"üöÄ CALLING MODEL: {target_model} ({provider_name}: {model_id})")
            
            # Log the actual request being sent (truncated, excluding base64 images)
            logger.debug(f"üì§ REQUEST MESSAGES: {len(message_dicts)} messages")
            for i, msg in enumerate(message_dicts):
                content = msg.get('content', '')
                if isinstance(content, list):
                    # Handle multimodal content
                    parts_summary = []
                    for part in content:
                        if part.get('type') == 'text':
                            text_preview = part.get('text', '')[:50]
                            parts_summary.append(f"text: '{text_preview}...'")
                        elif part.get('type') == 'image_url':
                            parts_summary.append("image: [base64_data]")
                        else:
                            parts_summary.append(f"{part.get('type', 'unknown')}: [data]")
                    content_preview = f"[{', '.join(parts_summary)}]"
                else:
                    content_preview = str(content)[:100]
                logger.debug(f"   Message {i+1}: {content_preview}...")
            
            response = await client.generate_response(
                model_id=model_id,
                messages=message_dicts,
                temperature=temperature,
                api_parameters=api_params,
                system_prompt=system_prompt
            )
            
            # Log response details
            if hasattr(response, 'choices') and response.choices:
                response_preview = response.choices[0].message.content[:100]
                logger.info(f"‚úÖ RESPONSE RECEIVED: '{response_preview}...'")
            elif isinstance(response, dict) and 'choices' in response:
                response_preview = response['choices'][0]['message']['content'][:100]
                logger.info(f"‚úÖ RESPONSE RECEIVED: '{response_preview}...'")
            
            logger.info(f"‚úÖ Generated response using {target_model} ({model_id})")
            return self._format_openai_response(response, target_model)
            
        except Exception as e:
            logger.error(f"‚ùå Error generating response with {target_model}: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"Failed to generate response: {str(e)}")
    
    def _format_openai_response(self, llm_response: Any, model_name: str) -> Dict[str, Any]:
        # Check if this is an image generation response
        if isinstance(llm_response, dict) and "data" in llm_response and llm_response.get("object") == "list":
            # This is an image generation response - return it as-is but ensure it's not treated as streaming
            logger.info(f"üñºÔ∏è Returning image generation response with {len(llm_response['data'])} images")
            
            # Convert to a format that works better with OpenWebUI streaming
            return {
                "id": llm_response.get("id", f"img-{__import__('uuid').uuid4()}"),
                "object": "chat.completion",  # Change from "list" to "chat.completion"
                "created": llm_response.get("created", int(__import__('time').time())),
                "model": model_name,
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": "I've generated an image based on your request."
                        },
                        "finish_reason": "stop"
                    }
                ],
                "usage": {"prompt_tokens": 0, "completion_tokens": 10, "total_tokens": 10},
                # Include the image data
                "images": llm_response.get("data", [])
            }
        
        # Check if this is an error response
        if isinstance(llm_response, dict) and "error" in llm_response:
            logger.error(f"‚ùå Returning error response: {llm_response['error']}")
            return llm_response
        
        # Standard chat completion response handling
        content = "An error occurred or no content was generated."
        usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        
        if hasattr(llm_response, 'choices') and llm_response.choices:
            content = llm_response.choices[0].message.content
            if hasattr(llm_response, 'usage'):
                usage = {
                    "prompt_tokens": getattr(llm_response.usage, 'prompt_tokens', 0),
                    "completion_tokens": getattr(llm_response.usage, 'completion_tokens', 0),
                    "total_tokens": getattr(llm_response.usage, 'total_tokens', 0)
                }
        elif isinstance(llm_response, dict):
            if 'choices' in llm_response:
                content = llm_response['choices'][0]['message']['content']
                usage = llm_response.get('usage', usage)
            elif 'message' in llm_response:
                content = llm_response['message']['content']
                usage = {
                    "prompt_tokens": llm_response.get('prompt_eval_count', 0),
                    "completion_tokens": llm_response.get('eval_count', 0),
                    "total_tokens": llm_response.get('prompt_eval_count', 0) + llm_response.get('eval_count', 0)
                }
        
        # Only add model prefix in debug mode and log it
        if settings.log_level.upper() in ["DEBUG"] and not content.startswith(f"{model_name} - "):
            content = f"{model_name} - {content}"
            logger.debug(f"üè∑Ô∏è ADDED MODEL PREFIX: {model_name}")

        return {
            "id": f"chatcmpl-{uuid.uuid4()}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model_name,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": content
                    },
                    "logprobs": None,
                    "finish_reason": "stop"
                }
            ],
            "usage": usage
        }

router = SmartRouter()
