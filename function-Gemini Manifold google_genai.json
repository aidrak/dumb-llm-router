[{"id":"gemini_manifold_google_genai","name":"Gemini Manifold google_genai","meta":{"description":"Manifold function for Gemini Developer API. Supports native image generation, grounding with Google Search and streaming. Uses google-genai.","manifest":{"title":"Gemini Manifold google_genai","id":"gemini_manifold_google_genai","description":"Manifold function for Gemini Developer API and Vertex AI. Uses the newer google-genai SDK. Aims to support as many features from it as possible.","author":"suurt8ll","author_url":"https://github.com/suurt8ll","funding_url":"https://github.com/suurt8ll/open_webui_functions","license":"MIT","version":"1.23.0","requirements":"google-genai==1.29.0"},"type":"pipe","user":{"id":"0a9c1a54-0894-4b9f-92b1-900d686caa83","username":"suurt8ll","name":"","createdAt":1728305531,"role":null,"verified":false},"id":"7e7bbc23-846e-455a-b810-172a8251d028"},"content":"\"\"\"\ntitle: Gemini Manifold google_genai\nid: gemini_manifold_google_genai\ndescription: Manifold function for Gemini Developer API and Vertex AI. Uses the newer google-genai SDK. Aims to support as many features from it as possible.\nauthor: suurt8ll\nauthor_url: https://github.com/suurt8ll\nfunding_url: https://github.com/suurt8ll/open_webui_functions\nlicense: MIT\nversion: 1.23.0\nrequirements: google-genai==1.29.0\n\"\"\"\n\n# Keys `title`, `id` and `description` in the frontmatter above are used for my own development purposes.\n# They don't have any effect on the plugin's functionality.\n\n\n# This is a helper function that provides a manifold for Google's Gemini Studio API and Vertex AI.\n# Be sure to check out my GitHub repository for more information! Contributions, questions and suggestions are very welcome.\n\nfrom google import genai\nfrom google.genai import types\nfrom google.genai import errors as genai_errors\nfrom google.cloud import storage\nfrom google.api_core import exceptions\n\nimport time\nimport copy\nimport json\nfrom urllib.parse import urlparse, parse_qs\nimport xxhash\nimport asyncio\nimport aiofiles\nfrom aiocache import cached\nfrom aiocache.base import BaseCache\nfrom aiocache.serializers import NullSerializer\nfrom aiocache.backends.memory import SimpleMemoryCache\nfrom functools import cache\nfrom datetime import datetime, timezone\nfrom fastapi.datastructures import State\nimport io\nimport mimetypes\nimport uuid\nimport base64\nimport re\nimport fnmatch\nimport sys\nfrom loguru import logger\nfrom fastapi import Request\nimport pydantic_core\nfrom pydantic import BaseModel, Field, field_validator\nfrom collections.abc import AsyncIterator, Awaitable, Callable\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Literal,\n    TYPE_CHECKING,\n    cast,\n)\n\nfrom open_webui.models.chats import Chats\nfrom open_webui.models.files import FileForm, Files\nfrom open_webui.storage.provider import Storage\nfrom open_webui.models.functions import Functions\nfrom open_webui.utils.misc import pop_system_message\n\nif TYPE_CHECKING:\n    from loguru import Record\n    from loguru._handler import Handler  # type: ignore\n    from utils.manifold_types import *  # My personal types in a separate file for more robustness.\n\n# Setting auditable=False avoids duplicate output for log levels that would be printed out by the main log.\nlog = logger.bind(auditable=False)\n\n# These tags will be \"disabled\" in the response, meaning that they will not be parsed by the backend.\nSPECIAL_TAGS_TO_DISABLE = [\n    \"details\",\n    \"think\",\n    \"thinking\",\n    \"reason\",\n    \"reasoning\",\n    \"thought\",\n    \"Thought\",\n    \"|begin_of_thought|\",\n    \"code_interpreter\",\n    \"|begin_of_solution|\",\n]\nZWS = \"\\u200b\"\n\n# region 1. Helper functions\n\n\nasync def emit_toast(\n    msg: str,\n    event_emitter: Callable[[\"Event\"], Awaitable[None]] | None,\n    toastType: Literal[\"info\", \"success\", \"warning\", \"error\"] = \"info\",\n) -> None:\n    \"\"\"Emits a toast notification to the front-end.\"\"\"\n\n    if not event_emitter:\n        return\n\n    event: \"NotificationEvent\" = {\n        \"type\": \"notification\",\n        \"data\": {\"type\": toastType, \"content\": msg},\n    }\n\n    try:\n        await event_emitter(event)\n    except Exception:\n        log.exception(\"Error emitting toast notification.\")\n\n\nasync def emit_status(\n    message: str,\n    event_emitter: Callable[[\"Event\"], Awaitable[None]] | None,\n    done: bool = False,\n    hidden: bool = False,\n) -> None:\n    \"\"\"Emit status updates asynchronously.\"\"\"\n\n    if not event_emitter:\n        return\n\n    status_event: \"StatusEvent\" = {\n        \"type\": \"status\",\n        \"data\": {\"description\": message, \"done\": done, \"hidden\": hidden},\n    }\n\n    try:\n        await event_emitter(status_event)\n        log.debug(f\"Emitted status:\", payload=status_event)\n    except Exception:\n        log.exception(\"Error emitting status.\")\n\n\nasync def emit_completion(\n    event_emitter: Callable[[\"Event\"], Awaitable[None]] | None,\n    content: str | None = None,\n    done: bool = False,\n    error: str | None = None,\n    sources: list[\"Source\"] | None = None,\n):\n    \"\"\"Constructs and emits completion event.\"\"\"\n\n    if not event_emitter:\n        return\n\n    emission: \"ChatCompletionEvent\" = {\n        \"type\": \"chat:completion\",\n        \"data\": {\"done\": done},\n    }\n    if content:\n        emission[\"data\"][\"content\"] = content\n    if error:\n        emission[\"data\"][\"error\"] = {\"detail\": error}\n    if sources:\n        emission[\"data\"][\"sources\"] = sources\n    await event_emitter(emission)\n\n\nasync def emit_error(\n    error_msg: str,\n    event_emitter: Callable[[\"Event\"], Awaitable[None]] | None,\n    warning: bool = False,\n    exception: bool = True,\n) -> None:\n    \"\"\"Emits an event to the front-end that causes it to display a nice red error message.\"\"\"\n\n    if warning:\n        log.opt(depth=1, exception=False).warning(error_msg)\n    else:\n        log.opt(depth=1, exception=exception).error(error_msg)\n    await emit_completion(\n        error=f\"\\n{error_msg}\", event_emitter=event_emitter, done=True\n    )\n\n\n# endregion 1. Helper functions\n\n\nclass GenaiApiError(Exception):\n    \"\"\"Custom exception for errors during Genai API interactions.\"\"\"\n\n    pass\n\n\nclass FilesAPIError(Exception):\n    \"\"\"Custom exception for errors during Files API operations.\"\"\"\n\n    pass\n\n\nclass UploadStatusManager:\n    \"\"\"\n    Manages and centralizes status updates for concurrent file uploads.\n\n    This manager is self-configuring. It discovers the number of files that\n    require an actual upload at runtime, only showing a status message to the\n    user when network activity is necessary.\n\n    The communication protocol uses tuples sent via an asyncio.Queue:\n    - ('REGISTER_UPLOAD',): Sent by a worker when it determines an upload is needed.\n    - ('COMPLETE_UPLOAD',): Sent by a worker when its upload is finished.\n    - ('FINALIZE',): Sent by the orchestrator when all workers are done.\n    \"\"\"\n\n    def __init__(self, event_emitter: Callable[[\"Event\"], Awaitable[None]] | None):\n        self.event_emitter = event_emitter\n        self.queue = asyncio.Queue()\n        self.total_uploads_expected = 0\n        self.uploads_completed = 0\n        self.finalize_received = False\n        self.is_active = False\n\n    async def run(self) -> None:\n        \"\"\"\n        Runs the manager loop, listening for updates and emitting status to the UI.\n        This should be started as a background task using asyncio.create_task().\n        \"\"\"\n        while not (\n            self.finalize_received\n            and self.total_uploads_expected == self.uploads_completed\n        ):\n            msg = await self.queue.get()\n            msg_type = msg[0]\n\n            if msg_type == \"REGISTER_UPLOAD\":\n                self.is_active = True\n                self.total_uploads_expected += 1\n                await self._emit_progress_update()\n            elif msg_type == \"COMPLETE_UPLOAD\":\n                self.uploads_completed += 1\n                await self._emit_progress_update()\n            elif msg_type == \"FINALIZE\":\n                self.finalize_received = True\n\n            self.queue.task_done()\n\n        log.debug(\"UploadStatusManager finished its run.\")\n\n    async def _emit_progress_update(self) -> None:\n        \"\"\"Emits the current progress to the front-end if uploads are active.\"\"\"\n        if not self.is_active:\n            return\n\n        is_done = (\n            self.total_uploads_expected > 0\n            and self.uploads_completed == self.total_uploads_expected\n        )\n        is_hidden = is_done\n\n        if is_done:\n            message = f\"Upload complete. {self.uploads_completed} file(s) processed.\"\n        else:\n            # Show \"Uploading 1 of N...\"\n            message = f\"Uploading file {self.uploads_completed + 1} of {self.total_uploads_expected}...\"\n\n        await emit_status(\n            message,\n            self.event_emitter,\n            done=is_done,\n            hidden=is_hidden,\n        )\n\n\nclass FilesAPIManager:\n    \"\"\"\n    Manages uploading, caching, and retrieving files using the Google Gemini Files API.\n\n    This class provides a stateless and efficient way to handle files by using a fast,\n    non-cryptographic hash (xxHash) of the file's content as the primary identifier.\n    This enables content-addressable storage, preventing duplicate uploads of the\n    same file. It uses a multi-tiered approach:\n\n    1. Hot Path (In-Memory Caches): For instantly retrieving file objects and hashes\n       for recently used files.\n    2. Warm Path (Stateless GET): For quickly recovering file state after a server\n       restart by using a deterministic name (derived from the content hash) and a\n       single `get` API call.\n    3. Cold Path (Upload): As a last resort, for uploading new files or re-uploading\n       expired ones.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: genai.Client,\n        file_cache: SimpleMemoryCache,\n        id_hash_cache: SimpleMemoryCache,\n        event_emitter: Callable[[\"Event\"], Awaitable[None]] | None,\n    ):\n        \"\"\"\n        Initializes the FilesAPIManager.\n\n        Args:\n            client: An initialized `google.genai.Client` instance.\n            file_cache: An aiocache instance for mapping `content_hash -> types.File`.\n                        Must be configured with `aiocache.serializers.NullSerializer`.\n            id_hash_cache: An aiocache instance for mapping `owui_file_id -> content_hash`.\n                           This is an optimization to avoid re-hashing known files.\n            event_emitter: A callable for emitting events to the front-end.\n        \"\"\"\n        self.client = client\n        self.file_cache = file_cache\n        self.id_hash_cache = id_hash_cache\n        self.event_emitter = event_emitter\n        # A dictionary to manage locks for concurrent uploads.\n        # The key is the content_hash, the value is an asyncio.Lock.\n        self.upload_locks: dict[str, asyncio.Lock] = {}\n\n    async def get_or_upload_file(\n        self,\n        file_bytes: bytes,\n        mime_type: str,\n        *,\n        owui_file_id: str | None = None,\n        status_queue: asyncio.Queue | None = None,\n    ) -> types.File:\n        \"\"\"\n        The main public method to get a file, using caching, recovery, or uploading.\n\n        This method uses a fast content hash (xxHash) as the primary key for all\n        caching and remote API interactions to ensure deduplication and performance.\n        It is safe from race conditions during concurrent uploads.\n\n        Args:\n            file_bytes: The raw byte content of the file. Required.\n            mime_type: The MIME type of the file (e.g., 'image/png'). Required.\n            owui_file_id: The unique ID of the file from Open WebUI, if available.\n                          Used for logging and as a key for the hash cache optimization.\n            status_queue: An optional asyncio.Queue to report upload lifecycle events.\n\n        Returns:\n            An `ACTIVE` `google.genai.types.File` object.\n\n        Raises:\n            FilesAPIError: If the file fails to upload or process.\n        \"\"\"\n        # Step 1: Get the fast content hash, using the ID cache as an optimization if possible.\n        content_hash = await self._get_content_hash(file_bytes, owui_file_id)\n\n        # Step 2: The Hot Path (Check Local File Cache)\n        # A cache hit means the file is valid and we can return immediately.\n        cached_file: types.File | None = await self.file_cache.get(content_hash)\n        if cached_file:\n            log_id = f\"OWUI ID: {owui_file_id}\" if owui_file_id else \"anonymous file\"\n            log.debug(\n                f\"Cache HIT for file hash {content_hash} ({log_id}). Returning immediately.\"\n            )\n            return cached_file\n\n        # On cache miss, acquire a lock specific to this file's content to prevent race conditions.\n        # dict.setdefault is atomic, ensuring only one lock is created per hash.\n        lock = self.upload_locks.setdefault(content_hash, asyncio.Lock())\n        if lock.locked():\n            log.debug(\n                f\"Lock for hash {content_hash} is held by another task. \"\n                f\"This call will now wait for the lock to be released.\"\n            )\n\n        async with lock:\n            # Step 2.5: Double-Checked Locking\n            # After acquiring the lock, check the cache again. Another task might have\n            # completed the upload while we were waiting for the lock.\n            cached_file = await self.file_cache.get(content_hash)\n            if cached_file:\n                log.debug(\n                    f\"Cache HIT for file hash {content_hash} after acquiring lock. Returning.\"\n                )\n                return cached_file\n\n            # Step 3: The Warm/Cold Path (On Cache Miss)\n            deterministic_name = f\"files/owui-v1-{content_hash}\"\n            log.debug(\n                f\"Cache MISS for hash {content_hash}. Attempting stateless recovery with GET: {deterministic_name}\"\n            )\n\n            try:\n                # Attempt to get the file (Warm Path)\n                file = await self.client.aio.files.get(name=deterministic_name)\n                if not file.name:\n                    raise FilesAPIError(\n                        f\"Stateless recovery for {deterministic_name} returned a file without a name.\"\n                    )\n\n                log.debug(\n                    f\"Stateless recovery successful for {deterministic_name}. File exists on server.\"\n                )\n                active_file = await self._poll_for_active_state(file.name, owui_file_id)\n\n                ttl_seconds = self._calculate_ttl(active_file.expiration_time)\n                await self.file_cache.set(content_hash, active_file, ttl=ttl_seconds)\n\n                return active_file\n            except genai_errors.ClientError as e:\n                if e.code == 403:  # \"Not found\" signal from the API.\n                    log.info(\n                        f\"File {deterministic_name} not found on server (received 403). Proceeding to upload.\"\n                    )\n                    # Proceed to upload (Cold Path)\n                    return await self._upload_and_process_file(\n                        content_hash,\n                        file_bytes,\n                        mime_type,\n                        deterministic_name,\n                        owui_file_id,\n                        status_queue,\n                    )\n                else:\n                    log.exception(\n                        f\"A non-403 client error occurred during stateless recovery for {deterministic_name}.\"\n                    )\n                    await emit_toast(\n                        f\"API error for file: {e.code}. Please check permissions.\",\n                        self.event_emitter,\n                        \"error\",\n                    )\n                    raise FilesAPIError(\n                        f\"Failed to check file status for {deterministic_name}: {e}\"\n                    ) from e\n            except Exception as e:\n                log.exception(\n                    f\"An unexpected error occurred during stateless recovery for {deterministic_name}.\"\n                )\n                await emit_toast(\n                    \"Unexpected error retrieving a file. Please try again.\",\n                    self.event_emitter,\n                    \"error\",\n                )\n                raise FilesAPIError(\n                    f\"Failed to check file status for {deterministic_name}: {e}\"\n                ) from e\n            finally:\n                # Clean up the lock from the dictionary once processing is complete\n                # for this hash, preventing memory growth over time.\n                # This is safe because any future request for this hash will hit the cache.\n                if content_hash in self.upload_locks:\n                    del self.upload_locks[content_hash]\n\n    async def _get_content_hash(\n        self, file_bytes: bytes, owui_file_id: str | None\n    ) -> str:\n        \"\"\"\n        Retrieves the file's content hash, using a cache for known IDs or computing it.\n\n        This acts as a memoization layer for the hashing process, avoiding\n        re-computation for files with a known Open WebUI ID. For anonymous files\n        (owui_file_id=None), it will always compute the hash.\n        \"\"\"\n        if owui_file_id:\n            # First, check the ID-to-Hash cache for known files.\n            cached_hash: str | None = await self.id_hash_cache.get(owui_file_id)\n            if cached_hash:\n                log.trace(f\"Hash cache HIT for OWUI ID {owui_file_id}.\")\n                return cached_hash\n\n        # If not in cache or if file is anonymous, compute the fast hash.\n        log.trace(\n            f\"Hash cache MISS for OWUI ID {owui_file_id if owui_file_id else 'N/A'}. Computing hash.\"\n        )\n        content_hash = xxhash.xxh64(file_bytes).hexdigest()\n\n        # If there was an ID, store the newly computed hash for next time.\n        if owui_file_id:\n            await self.id_hash_cache.set(owui_file_id, content_hash)\n\n        return content_hash\n\n    def _calculate_ttl(self, expiration_time: datetime | None) -> float | None:\n        \"\"\"Calculates the TTL in seconds from an expiration datetime.\"\"\"\n        if not expiration_time:\n            return None\n\n        now_utc = datetime.now(timezone.utc)\n        if expiration_time <= now_utc:\n            return 0\n\n        return (expiration_time - now_utc).total_seconds()\n\n    async def _upload_and_process_file(\n        self,\n        content_hash: str,\n        file_bytes: bytes,\n        mime_type: str,\n        deterministic_name: str,\n        owui_file_id: str | None,\n        status_queue: asyncio.Queue | None = None,\n    ) -> types.File:\n        \"\"\"Handles the full upload and post-upload processing workflow.\"\"\"\n\n        # Register with the manager that an actual upload is starting.\n        if status_queue:\n            await status_queue.put((\"REGISTER_UPLOAD\",))\n\n        log.info(f\"Starting upload for {deterministic_name}...\")\n\n        try:\n            file_io = io.BytesIO(file_bytes)\n            upload_config = types.UploadFileConfig(\n                name=deterministic_name, mime_type=mime_type\n            )\n            uploaded_file = await self.client.aio.files.upload(\n                file=file_io, config=upload_config\n            )\n            if not uploaded_file.name:\n                raise FilesAPIError(\n                    f\"File upload for {deterministic_name} did not return a file name.\"\n                )\n\n            log.debug(f\"{uploaded_file.name} uploaded.\")\n            log.trace(\"Uploaded file details:\", payload=uploaded_file)\n\n            # Check if the file is already active. If so, we can skip polling.\n            if uploaded_file.state == types.FileState.ACTIVE:\n                log.debug(\n                    f\"File {uploaded_file.name} is already ACTIVE. Skipping poll.\"\n                )\n                active_file = uploaded_file\n            else:\n                # If not active, proceed with the original polling logic.\n                log.debug(\n                    f\"{uploaded_file.name} uploaded with state {uploaded_file.state}. Polling for ACTIVE state.\"\n                )\n                active_file = await self._poll_for_active_state(\n                    uploaded_file.name, owui_file_id\n                )\n                log.debug(f\"File {active_file.name} is now ACTIVE.\")\n\n            # Calculate TTL and set in the main file cache using the content hash as the key.\n            ttl_seconds = self._calculate_ttl(active_file.expiration_time)\n            await self.file_cache.set(content_hash, active_file, ttl=ttl_seconds)\n            log.debug(\n                f\"Cached new file object for hash {content_hash} with TTL: {ttl_seconds}s.\"\n            )\n\n            return active_file\n        except Exception as e:\n            log.exception(f\"File upload or processing failed for {deterministic_name}.\")\n            await emit_toast(\n                \"Upload failed for a file. Please check connection and try again.\",\n                self.event_emitter,\n                \"error\",\n            )\n            raise FilesAPIError(f\"Upload failed for {deterministic_name}: {e}\") from e\n        finally:\n            # Report completion (success or failure) to the status manager.\n            # This ensures the progress counter always advances.\n            if status_queue:\n                await status_queue.put((\"COMPLETE_UPLOAD\",))\n\n    async def _poll_for_active_state(\n        self,\n        file_name: str,\n        owui_file_id: str | None,\n        timeout: int = 60,\n        poll_interval: int = 1,\n    ) -> types.File:\n        \"\"\"Polls the file's status until it is ACTIVE or fails.\"\"\"\n        end_time = time.monotonic() + timeout\n        while time.monotonic() < end_time:\n            try:\n                file = await self.client.aio.files.get(name=file_name)\n            except Exception as e:\n                raise FilesAPIError(\n                    f\"Polling failed: Could not get status for {file_name}. Reason: {e}\"\n                ) from e\n\n            if file.state == types.FileState.ACTIVE:\n                return file\n            if file.state == types.FileState.FAILED:\n                log_id = f\"'{owui_file_id}'\" if owui_file_id else \"an uploaded file\"\n                error_message = f\"File processing failed on server for {file_name}.\"\n                toast_message = f\"Google could not process {log_id}.\"\n                if file.error:\n                    reason = f\"Reason: {file.error.message} (Code: {file.error.code})\"\n                    error_message += f\" {reason}\"\n                    toast_message += f\" Reason: {file.error.message}\"\n\n                await emit_toast(toast_message, self.event_emitter, \"error\")\n                raise FilesAPIError(error_message)\n\n            state_name = file.state.name if file.state else \"UNKNOWN\"\n            log.trace(\n                f\"File {file_name} is still {state_name}. Waiting {poll_interval}s...\"\n            )\n            await asyncio.sleep(poll_interval)\n\n        raise FilesAPIError(\n            f\"File {file_name} did not become ACTIVE within {timeout} seconds.\"\n        )\n\n\nclass GeminiContentBuilder:\n\n    def __init__(\n        self,\n        messages_body: list[\"Message\"],\n        metadata_body: \"Metadata\",\n        user_data: \"UserData\",\n        event_emitter: Callable[[\"Event\"], Awaitable[None]] | None,\n        valves: \"Pipe.Valves\",\n        files_api_manager: \"FilesAPIManager\",\n    ):\n        self.messages_body = messages_body\n        self.upload_documents = (metadata_body.get(\"features\", {}) or {}).get(\n            \"upload_documents\", False\n        )\n        self.event_emitter = event_emitter\n        self.valves = valves\n        self.files_api_manager = files_api_manager\n        self.is_temp_chat = metadata_body.get(\"chat_id\") == \"local\"\n        self.vertexai = self.files_api_manager.client.vertexai\n\n        self.system_prompt, self.messages_body = self._extract_system_prompt(\n            self.messages_body\n        )\n        self.messages_db = self._fetch_and_validate_chat_history(\n            metadata_body, user_data\n        )\n\n    async def build_contents(self) -> list[types.Content]:\n        \"\"\"\n        The main public method to generate the contents list by processing all\n        message turns concurrently and using a self-configuring status manager.\n        \"\"\"\n        if not self.messages_db:\n            warn_msg = (\n                \"There was a problem retrieving the messages from the backend database. \"\n                \"Check the console for more details. \"\n                \"Citation filtering and file uploads will not be available.\"\n            )\n            await emit_toast(warn_msg, self.event_emitter, \"warning\")\n\n        # 1. Set up and launch the status manager. It will activate itself if needed.\n        status_manager = UploadStatusManager(self.event_emitter)\n        manager_task = asyncio.create_task(status_manager.run())\n\n        # 2. Create and run concurrent processing tasks for each message turn.\n        tasks = [\n            self._process_message_turn(i, message, status_manager.queue)\n            for i, message in enumerate(self.messages_body)\n        ]\n        log.debug(f\"Starting concurrent processing of {len(tasks)} message turns.\")\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        # 3. Signal to the manager that no more uploads will be registered.\n        await status_manager.queue.put((\"FINALIZE\",))\n\n        # 4. Wait for the manager to finish processing all reported uploads.\n        await manager_task\n\n        # 5. Filter and assemble the final contents list.\n        contents: list[types.Content] = []\n        for i, res in enumerate(results):\n            if isinstance(res, types.Content):\n                contents.append(res)\n            elif isinstance(res, Exception):\n                log.error(\n                    f\"An error occurred while processing message {i} concurrently.\",\n                    payload=res,\n                )\n        return contents\n\n    @staticmethod\n    def _extract_system_prompt(\n        messages: list[\"Message\"],\n    ) -> tuple[str | None, list[\"Message\"]]:\n        \"\"\"Extracts the system prompt and returns it along with the modified message list.\"\"\"\n        system_message, remaining_messages = pop_system_message(messages)  # type: ignore\n        system_prompt: str | None = (system_message or {}).get(\"content\")\n        return system_prompt, remaining_messages  # type: ignore\n\n    def _fetch_and_validate_chat_history(\n        self, metadata_body: \"Metadata\", user_data: \"UserData\"\n    ) -> list[\"ChatMessageTD\"] | None:\n        \"\"\"\n        Fetches message history from the database and validates its length against the request body.\n        Returns the database messages or None if not found or if validation fails.\n        \"\"\"\n        # 1. Fetch from database\n        chat_id = metadata_body.get(\"chat_id\", \"\")\n        if chat := Chats.get_chat_by_id_and_user_id(\n            id=chat_id, user_id=user_data[\"id\"]\n        ):\n            chat_content: \"ChatObjectDataTD\" = chat.chat  # type: ignore\n            # Last message is the upcoming assistant response, at this point in the logic it's empty.\n            messages_db = chat_content.get(\"messages\", [])[:-1]\n        else:\n            log.warning(\n                f\"Chat {chat_id} not found. Cannot process files or filter citations.\"\n            )\n            return None\n\n        # 2. Validate length against the current message body\n        if len(messages_db) != len(self.messages_body):\n            warn_msg = (\n                f\"Messages in the body ({len(self.messages_body)}) and \"\n                f\"messages in the database ({len(messages_db)}) do not match. \"\n                \"This is likely due to a bug in Open WebUI. \"\n                \"Cannot process files or filter citations.\"\n            )\n\n            # TODO: Emit a toast to the user in the front-end.\n            log.warning(warn_msg)\n            # Invalidate the db messages if they don't match\n            return None\n\n        return messages_db\n\n    async def _process_message_turn(\n        self, i: int, message: \"Message\", status_queue: asyncio.Queue\n    ) -> types.Content | None:\n        \"\"\"\n        Processes a single message turn, handling user and assistant roles,\n        and returns a complete `types.Content` object. Designed to be run concurrently.\n        \"\"\"\n        role = message.get(\"role\")\n        parts: list[types.Part] = []\n\n        if role == \"user\":\n            message = cast(\"UserMessage\", message)\n            files = []\n            if self.messages_db:\n                message_db = self.messages_db[i]\n                if self.upload_documents:\n                    files = message_db.get(\"files\", [])\n            parts = await self._process_user_message(\n                message, files, self.event_emitter, status_queue\n            )\n            # Case 1: User content is completely empty (no text, no files).\n            if not parts:\n                log.info(\n                    f\"User message at index {i} is completely empty. \"\n                    \"Injecting a prompt to ask for clarification.\"\n                )\n                # Inform the user via a toast notification.\n                toast_msg = f\"Your message #{i + 1} was empty. The assistant will ask for clarification.\"\n                await emit_toast(toast_msg, self.event_emitter, \"info\")\n\n                clarification_prompt = (\n                    \"The user sent an empty message. Please ask the user for \"\n                    \"clarification on what they would like to ask or discuss.\"\n                )\n                # This will become the only part for this user message.\n                parts = await self._genai_parts_from_text(\n                    clarification_prompt, status_queue\n                )\n            else:\n                # Case 2: User has sent content, check if it includes text.\n                has_text_component = any(p.text for p in parts if p.text)\n                if not has_text_component:\n                    # The user sent content (e.g., files) but no accompanying text.\n                    if self.vertexai:\n                        # Vertex AI requires a text part in multi-modal messages.\n                        log.info(\n                            f\"User message at index {i} lacks a text component for Vertex AI. \"\n                            \"Adding default text prompt.\"\n                        )\n                        # Inform the user via a toast notification.\n                        toast_msg = (\n                            f\"For your message #{i + 1}, a default prompt was added as text is required \"\n                            \"for requests with attachments when using Vertex AI.\"\n                        )\n                        await emit_toast(toast_msg, self.event_emitter, \"info\")\n\n                        default_prompt_text = (\n                            \"The user did not send any text message with the additional context. \"\n                            \"Answer by summarizing the newly added context.\"\n                        )\n                        default_text_parts = await self._genai_parts_from_text(\n                            default_prompt_text, status_queue\n                        )\n                        parts.extend(default_text_parts)\n                    else:\n                        # Google Developer API allows no-text user content.\n                        log.info(\n                            f\"User message at index {i} lacks a text component for Google Developer API. \"\n                            \"Proceeding with non-text parts only.\"\n                        )\n        elif role == \"assistant\":\n            message = cast(\"AssistantMessage\", message)\n            # Google API's assistant role is \"model\"\n            role = \"model\"\n            sources = None\n            if self.messages_db:\n                message_db = self.messages_db[i]\n                sources = message_db.get(\"sources\")\n            parts = await self._process_assistant_message(\n                message, sources, status_queue\n            )\n        else:\n            warn_msg = f\"Message {i} has an invalid role: {role}. Skipping to the next message.\"\n            log.warning(warn_msg)\n            await emit_toast(warn_msg, self.event_emitter, \"warning\")\n            return None\n\n        # Only create a Content object if there are parts to include.\n        if parts:\n            return types.Content(parts=parts, role=role)\n        return None\n\n    async def _process_user_message(\n        self,\n        message: \"UserMessage\",\n        files: list[\"FileAttachmentTD\"],\n        event_emitter: Callable[[\"Event\"], Awaitable[None]] | None,\n        status_queue: asyncio.Queue,\n    ) -> list[types.Part]:\n        user_parts: list[types.Part] = []\n        db_files_processed = False\n\n        # PATH 1: Database is available (Normal Chat).\n        if self.messages_db and files:\n            db_files_processed = True\n            log.info(f\"Processing {len(files)} files from the database concurrently.\")\n\n            upload_tasks = []\n            for file in files:\n                log.debug(\"Preparing DB file for concurrent upload:\", payload=file)\n                uri = \"\"\n                if file.get(\"type\") == \"image\":\n                    uri = file.get(\"url\", \"\")\n                elif file.get(\"type\") == \"file\":\n                    # Reconstruct the local API URI to be handled by our unified function\n                    uri = f\"/api/v1/files/{file.get('id', '')}/content\"\n\n                if uri:\n                    # Create a coroutine for each file upload and add it to a list.\n                    upload_tasks.append(self._genai_part_from_uri(uri, status_queue))\n                else:\n                    log.warning(\"Could not determine URI for file in DB.\", payload=file)\n\n            if upload_tasks:\n                # Run all upload tasks concurrently. asyncio.gather maintains the order of results.\n                results = await asyncio.gather(*upload_tasks)\n                # Filter out None results (from failed uploads) and add the successful parts to the list.\n                user_parts.extend(part for part in results if part)\n\n        # Now, process the content from the message payload.\n        user_content = message.get(\"content\")\n        if isinstance(user_content, str):\n            user_content_list: list[\"Content\"] = [\n                {\"type\": \"text\", \"text\": user_content}\n            ]\n        elif isinstance(user_content, list):\n            user_content_list = user_content\n        else:\n            warn_msg = \"User message content is not a string or list, skipping.\"\n            log.warning(warn_msg)\n            await emit_toast(warn_msg, event_emitter, \"warning\")\n            return user_parts\n\n        for c in user_content_list:\n            c_type = c.get(\"type\")\n            if c_type == \"text\":\n                c = cast(\"TextContent\", c)\n                if c_text := c.get(\"text\"):\n                    user_parts.extend(\n                        await self._genai_parts_from_text(c_text, status_queue)\n                    )\n\n            # PATH 2: Temporary Chat Image Handling.\n            elif c_type == \"image_url\" and not db_files_processed:\n                log.info(\"Processing image from payload (temporary chat mode).\")\n                c = cast(\"ImageContent\", c)\n                if uri := c.get(\"image_url\", {}).get(\"url\"):\n                    if part := await self._genai_part_from_uri(uri, status_queue):\n                        user_parts.append(part)\n\n        return user_parts\n\n    async def _process_assistant_message(\n        self,\n        message: \"AssistantMessage\",\n        sources: list[\"Source\"] | None,\n        status_queue: asyncio.Queue,\n    ) -> list[types.Part]:\n        assistant_text = message.get(\"content\")\n        if sources:\n            assistant_text = self._remove_citation_markers(assistant_text, sources)\n        return await self._genai_parts_from_text(assistant_text, status_queue)\n\n    async def _genai_part_from_uri(\n        self, uri: str, status_queue: asyncio.Queue\n    ) -> types.Part | None:\n        \"\"\"\n        Processes any resource URI and returns a genai.types.Part.\n        This is the central dispatcher for all media processing, handling data URIs,\n        local API file paths, and YouTube URLs. It decides whether to use the\n        Files API or send raw bytes based on configuration and context.\n        \"\"\"\n        if not uri:\n            log.warning(\"Received an empty URI, skipping.\")\n            return None\n\n        try:\n            file_bytes: bytes | None = None\n            mime_type: str | None = None\n            owui_file_id: str | None = None\n\n            # Step 1: Extract bytes and mime_type from the URI if applicable\n            if uri.startswith(\"data:image\"):\n                match = re.match(r\"data:(image/\\w+);base64,(.+)\", uri)\n                if not match:\n                    raise ValueError(\"Invalid data URI for image.\")\n                mime_type, base64_data = match.group(1), match.group(2)\n                file_bytes = base64.b64decode(base64_data)\n            elif uri.startswith(\"/api/v1/files/\"):\n                log.info(f\"Processing local API file URI: {uri}\")\n                file_id = uri.split(\"/\")[4]\n                owui_file_id = file_id\n                file_bytes, mime_type = await self._get_file_data(file_id)\n            elif \"youtube.com/\" in uri or \"youtu.be/\" in uri:\n                log.info(f\"Found YouTube URL: {uri}\")\n                return self._genai_part_from_youtube_uri(uri)\n            # TODO: Google Cloud Storage bucket support.\n            # elif uri.startswith(\"gs://\"): ...\n            else:\n                warn_msg = f\"Unsupported URI: '{uri[:64]}...' Links must be to YouTube or a supported file type.\"\n                log.warning(warn_msg)\n                await emit_toast(warn_msg, self.event_emitter, \"warning\")\n                return None\n\n            # Step 2: If we have bytes, decide how to create the Part\n            if file_bytes and mime_type:\n                # TODO: The Files API is strict about MIME types (e.g., text/plain,\n                # application/pdf). In the future, inspect the content of files\n                # with unsupported text-like MIME types (e.g., 'application/json',\n                # 'text/markdown'). If the content is detected as plaintext,\n                # override the `mime_type` variable to 'text/plain' to allow the upload.\n\n                # Determine whether to use the Files API based on the specified conditions.\n                use_files_api = True\n                reason = \"\"\n\n                if not self.valves.USE_FILES_API:\n                    reason = \"disabled by user setting (USE_FILES_API=False)\"\n                    use_files_api = False\n                elif self.vertexai:\n                    reason = \"the active client is configured for Vertex AI, which does not support the Files API\"\n                    use_files_api = False\n                elif self.is_temp_chat:\n                    reason = \"temporary chat mode is active\"\n                    use_files_api = False\n\n                if use_files_api:\n                    log.info(f\"Using Files API for resource from URI: {uri[:64]}...\")\n                    gemini_file = await self.files_api_manager.get_or_upload_file(\n                        file_bytes=file_bytes,\n                        mime_type=mime_type,\n                        owui_file_id=owui_file_id,\n                        status_queue=status_queue,\n                    )\n                    return types.Part(\n                        file_data=types.FileData(\n                            file_uri=gemini_file.uri,\n                            mime_type=gemini_file.mime_type,\n                        )\n                    )\n                else:\n                    log.info(\n                        f\"Sending raw bytes because {reason}. Resource from URI: {uri[:64]}...\"\n                    )\n                    return types.Part.from_bytes(data=file_bytes, mime_type=mime_type)\n\n            return None  # Return None if bytes/mime_type could not be determined\n\n        except FilesAPIError as e:\n            error_msg = f\"Files API failed for URI '{uri[:64]}...': {e}\"\n            log.error(error_msg)\n            await emit_toast(error_msg, self.event_emitter, \"error\")\n            return None\n        except Exception:\n            log.exception(f\"Error processing URI: {uri[:64]}[...]\")\n            return None\n\n    def _genai_part_from_youtube_uri(self, uri: str) -> types.Part | None:\n        \"\"\"Creates a Gemini Part from a YouTube URL, with optional video metadata.\n\n        Handles standard (`watch?v=`), short (`youtu.be/`), mobile (`shorts/`),\n        and live (`live/`) URLs. Metadata is parsed for the Gemini Developer API\n        but ignored for Vertex AI, which receives a simple URI Part.\n\n        - **Start/End Time**: `?t=<value>` and `#end=<value>`. The value can be a\n          flexible duration (e.g., \"1m30s\", \"90\") and will be converted to seconds.\n        - **Frame Rate**: Can be specified in two ways (if both are present,\n          `interval` takes precedence):\n          - **Interval**: `#interval=<value>` (e.g., `#interval=10s`, `#interval=0.5s`).\n            The value is a flexible duration converted to seconds, then to FPS (1/interval).\n          - **FPS**: `#fps=<value>` (e.g., `#fps=2.5`).\n          The final FPS value must be in the range (0, 24].\n\n        Args:\n            uri: The raw YouTube URL from the user.\n            is_vertex_client: If True, creates a simple Part for Vertex AI.\n\n        Returns:\n            A `types.Part` object, or `None` if the URI is not a valid YouTube link.\n        \"\"\"\n        # Convert YouTube Music URLs to standard YouTube URLs for consistent parsing.\n        if \"music.youtube.com\" in uri:\n            uri = uri.replace(\"music.youtube.com\", \"www.youtube.com\")\n            log.info(f\"Converted YouTube Music URL to standard URL: {uri}\")\n\n        # Regex to capture the 11-character video ID from various YouTube URL formats.\n        video_id_pattern = re.compile(\n            r\"(?:https?://)?(?:www\\.)?(?:youtube\\.com/(?:watch\\?v=|shorts/|live/)|youtu.be/)([a-zA-Z0-9_-]{11})\"\n        )\n\n        match = video_id_pattern.search(uri)\n        if not match:\n            log.warning(f\"Could not extract a valid YouTube video ID from URI: {uri}\")\n            return None\n\n        video_id = match.group(1)\n        canonical_uri = f\"https://www.youtube.com/watch?v={video_id}\"\n\n        # --- Branching logic for Vertex AI vs. Gemini Developer API ---\n        if self.vertexai:\n            return types.Part.from_uri(file_uri=canonical_uri, mime_type=\"video/mp4\")\n        else:\n            parsed_uri = urlparse(uri)\n            query_params = parse_qs(parsed_uri.query)\n            fragment_params = parse_qs(parsed_uri.fragment)\n\n            start_offset: str | None = None\n            end_offset: str | None = None\n            fps: float | None = None\n\n            # Start time from query `t`. Convert flexible format to \"Ns\".\n            if \"t\" in query_params:\n                raw_start = query_params[\"t\"][0]\n                if (\n                    total_seconds := self._parse_duration_to_seconds(raw_start)\n                ) is not None:\n                    start_offset = f\"{total_seconds}s\"\n\n            # End time from fragment `end`. Convert flexible format to \"Ns\".\n            if \"end\" in fragment_params:\n                raw_end = fragment_params[\"end\"][0]\n                if (\n                    total_seconds := self._parse_duration_to_seconds(raw_end)\n                ) is not None:\n                    end_offset = f\"{total_seconds}s\"\n\n            # Frame rate from fragment `interval` or `fps`. `interval` takes precedence.\n            if \"interval\" in fragment_params:\n                raw_interval = fragment_params[\"interval\"][0]\n                if (\n                    interval_seconds := self._parse_duration_to_seconds(raw_interval)\n                ) is not None and interval_seconds > 0:\n                    calculated_fps = 1.0 / interval_seconds\n                    if 0.0 < calculated_fps <= 24.0:\n                        fps = calculated_fps\n                    else:\n                        log.warning(\n                            f\"Interval '{raw_interval}' results in FPS '{calculated_fps}' which is outside the valid range (0.0, 24.0]. Ignoring.\"\n                        )\n\n            # Fall back to `fps` param if not set by `interval`.\n            if fps is None and \"fps\" in fragment_params:\n                try:\n                    fps_val = float(fragment_params[\"fps\"][0])\n                    if 0.0 < fps_val <= 24.0:\n                        fps = fps_val\n                    else:\n                        log.warning(\n                            f\"FPS value '{fps_val}' is outside the valid range (0.0, 24.0]. Ignoring.\"\n                        )\n                except (ValueError, IndexError):\n                    log.warning(\n                        f\"Invalid FPS value in fragment: {fragment_params.get('fps')}. Ignoring.\"\n                    )\n\n            video_metadata: types.VideoMetadata | None = None\n            if start_offset or end_offset or fps is not None:\n                video_metadata = types.VideoMetadata(\n                    start_offset=start_offset,\n                    end_offset=end_offset,\n                    fps=fps,\n                )\n\n            return types.Part(\n                file_data=types.FileData(file_uri=canonical_uri),\n                video_metadata=video_metadata,\n            )\n\n    def _parse_duration_to_seconds(self, duration_str: str) -> float | None:\n        \"\"\"Converts a human-readable duration string to total seconds.\n\n        Supports formats like \"1h30m15s\", \"90m\", \"3600s\", or just \"90\".\n        Also supports float values like \"0.5s\" or \"90.5\".\n        Returns total seconds as a float, or None if the string is invalid.\n        \"\"\"\n        # First, try to convert the whole string as a plain number (e.g., \"90\", \"90.5\").\n        try:\n            return float(duration_str)\n        except ValueError:\n            # If it fails, it might be a composite duration like \"1m30s\", so we parse it below.\n            pass\n\n        total_seconds = 0.0\n        # Regex to find number-unit pairs (e.g., 1h, 30.5m, 15s). Supports floats.\n        parts = re.findall(r\"(\\d+(?:\\.\\d+)?)\\s*(h|m|s)?\", duration_str, re.IGNORECASE)\n\n        if not parts:\n            # log.warning(f\"Could not parse duration string: {duration_str}\")\n            return None\n\n        for value, unit in parts:\n            val = float(value)\n            unit = (unit or \"s\").lower()  # Default to seconds if no unit\n            if unit == \"h\":\n                total_seconds += val * 3600\n            elif unit == \"m\":\n                total_seconds += val * 60\n            elif unit == \"s\":\n                total_seconds += val\n\n        return total_seconds\n\n    @staticmethod\n    def _enable_special_tags(text: str) -> str:\n        \"\"\"\n        Reverses the action of _disable_special_tags by removing the ZWS\n        from special tags. This is used to clean up history messages before\n        sending them to the model, so it can understand the context correctly.\n        \"\"\"\n        if not text:\n            return \"\"\n\n        # The regex finds '<ZWS' followed by an optional '/' and then one of the special tags.\n        # The inner parentheses group the tags, so the optional '/' applies to all of them.\n        REVERSE_TAG_REGEX = re.compile(\n            r\"<\"\n            + ZWS\n            + r\"(/?\"\n            + \"(\"\n            + \"|\".join(re.escape(tag) for tag in SPECIAL_TAGS_TO_DISABLE)\n            + \")\"\n            + r\")\"\n        )\n        # The substitution restores the original tag, e.g., '<ZWS/think' becomes '</think'.\n        restored_text, count = REVERSE_TAG_REGEX.subn(r\"<\\1\", text)\n        if count > 0:\n            log.debug(f\"Re-enabled {count} special tag(s) for model context.\")\n\n        return restored_text\n\n    async def _genai_parts_from_text(\n        self, text: str, status_queue: asyncio.Queue\n    ) -> list[types.Part]:\n        if not text:\n            return []\n\n        text = self._enable_special_tags(text)\n        parts: list[types.Part] = []\n        last_pos = 0\n\n        # Conditionally build a regex to find media links.\n        # If YouTube parsing is disabled, the regex will only find markdown image links,\n        # leaving YouTube URLs to be treated as plain text.\n        markdown_part = r\"!\\[.*?\\]\\(([^)]+)\\)\"  # Group 1: Markdown URI\n        youtube_part = r\"(https?://(?:(?:www|music)\\.)?youtube\\.com/(?:watch\\?v=|shorts/|live/)[^\\s)]+|https?://youtu\\.be/[^\\s)]+)\"  # Group 2: YouTube URL\n        if self.valves.PARSE_YOUTUBE_URLS:\n            pattern = re.compile(f\"{markdown_part}|{youtube_part}\")\n            process_youtube = True\n        else:\n            pattern = re.compile(markdown_part)\n            process_youtube = False\n            log.info(\n                \"YouTube URL parsing is disabled. URLs will be treated as plain text.\"\n            )\n\n        for match in pattern.finditer(text):\n            # Add the text segment that precedes the media link\n            if text_segment := text[last_pos : match.start()].strip():\n                parts.append(types.Part.from_text(text=text_segment))\n\n            # The URI is in group 1 for markdown, or group 2 for YouTube.\n            if process_youtube:\n                uri = match.group(1) or match.group(2)\n            else:\n                uri = match.group(1)\n\n            if not uri:\n                log.warning(\n                    f\"Found unsupported URI format in text: {match.group(0)}. Skipping.\"\n                )\n                continue\n\n            # Delegate all URI processing to the unified helper\n            if media_part := await self._genai_part_from_uri(uri, status_queue):\n                parts.append(media_part)\n\n            last_pos = match.end()\n\n        # Add any remaining text after the last media link\n        if remaining_text := text[last_pos:].strip():\n            parts.append(types.Part.from_text(text=remaining_text))\n\n        # If no media links were found, the whole text is a single part\n        if not parts and text.strip():\n            parts.append(types.Part.from_text(text=text.strip()))\n\n        return parts\n\n    @staticmethod\n    async def _get_file_data(file_id: str) -> tuple[bytes | None, str | None]:\n        \"\"\"\n        Asynchronously retrieves file metadata from the database and its content from disk.\n        \"\"\"\n        # TODO: Emit toasts on unexpected conditions.\n        if not file_id:\n            log.warning(\"file_id is empty. Cannot continue.\")\n            return None, None\n\n        # Run the synchronous, blocking database call in a separate thread\n        # to avoid blocking the main asyncio event loop.\n        try:\n            file_model = await asyncio.to_thread(Files.get_file_by_id, file_id)\n        except Exception as e:\n            log.exception(\n                f\"An unexpected error occurred during database call for file_id {file_id}: {e}\"\n            )\n            return None, None\n\n        if file_model is None:\n            # The get_file_by_id method already handles and logs the specific exception,\n            # so we just need to handle the None return value.\n            log.warning(f\"File {file_id} not found in the backend's database.\")\n            return None, None\n\n        if not (file_path := file_model.path):\n            log.warning(\n                f\"File {file_id} was found in the database but it lacks `path` field. Cannot Continue.\"\n            )\n            return None, None\n        if file_model.meta is None:\n            log.warning(\n                f\"File {file_path} was found in the database but it lacks `meta` field. Cannot continue.\"\n            )\n            return None, None\n        if not (content_type := file_model.meta.get(\"content_type\")):\n            log.warning(\n                f\"File {file_path} was found in the database but it lacks `meta.content_type` field. Cannot continue.\"\n            )\n            return None, None\n\n        if file_path.startswith(\"gs://\"):\n            try:\n                # Initialize the GCS client\n                storage_client = storage.Client()\n\n                # Parse the GCS path\n                # The path should be in the format \"gs://bucket-name/object-name\"\n                if len(file_path.split(\"/\", 3)) < 4:\n                    raise ValueError(\n                        f\"Invalid GCS path: '{file_path}'. \"\n                        \"Path must be in the format 'gs://bucket-name/object-name'.\"\n                    )\n\n                bucket_name, blob_name = file_path.removeprefix(\"gs://\").split(\"/\", 1)\n\n                # Get the bucket and blob (file object)\n                bucket = storage_client.bucket(bucket_name)\n                blob = bucket.blob(blob_name)\n\n                # Download the file's content as bytes\n                print(f\"Reading from GCS: {file_path}\")\n                return blob.download_as_bytes(), content_type\n            except exceptions.NotFound:\n                print(f\"Error: GCS object not found at {file_path}\")\n                raise\n            except Exception as e:\n                print(f\"An error occurred while reading from GCS: {e}\")\n                raise\n        try:\n            async with aiofiles.open(file_path, \"rb\") as file:\n                file_data = await file.read()\n            return file_data, content_type\n        except FileNotFoundError:\n            log.exception(f\"File {file_path} not found on disk.\")\n            return None, content_type\n        except Exception:\n            log.exception(f\"Error processing file {file_path}\")\n            return None, content_type\n\n    @staticmethod\n    def _remove_citation_markers(text: str, sources: list[\"Source\"]) -> str:\n        original_text = text\n        processed: set[str] = set()\n        for source in sources:\n            supports = [\n                metadata[\"supports\"]\n                for metadata in source.get(\"metadata\", [])\n                if \"supports\" in metadata\n            ]\n            supports = [item for sublist in supports for item in sublist]\n            for support in supports:\n                support = types.GroundingSupport(**support)\n                indices = support.grounding_chunk_indices\n                segment = support.segment\n                if not (indices and segment):\n                    continue\n                segment_text = segment.text\n                if not segment_text:\n                    continue\n                # Using a shortened version because user could edit the assistant message in the front-end.\n                # If citation segment get's edited, then the markers would not be removed. Shortening reduces the\n                # chances of this happening.\n                segment_end = segment_text[-32:]\n                if segment_end in processed:\n                    continue\n                processed.add(segment_end)\n                citation_markers = \"\".join(f\"[{index + 1}]\" for index in indices)\n                # Find the position of the citation markers in the text\n                pos = text.find(segment_text + citation_markers)\n                if pos != -1:\n                    # Remove the citation markers\n                    text = (\n                        text[: pos + len(segment_text)]\n                        + text[pos + len(segment_text) + len(citation_markers) :]\n                    )\n        trim = len(original_text) - len(text)\n        log.debug(\n            f\"Citation removal finished. Returning text str that is {trim} character shorter than the original input.\"\n        )\n        return text\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        GEMINI_API_KEY: str | None = Field(default=None)\n        USER_MUST_PROVIDE_AUTH_CONFIG: bool = Field(\n            default=False,\n            description=\"\"\"Whether to require users (including admins) to provide their own authentication configuration.\n            User can provide these through UserValves. Setting this to True will disallow users from using Vertex AI.\n            Default value is False.\"\"\",\n        )\n        AUTH_WHITELIST: str | None = Field(\n            default=None,\n            description=\"\"\"Comma separated list of user emails that are allowed to bypassUSER_MUST_PROVIDE_AUTH_CONFIG and use the default authentication configuration.\n            Default value is None (no users are whitelisted).\"\"\",\n        )\n        GEMINI_API_BASE_URL: str | None = Field(\n            default=None,\n            description=\"\"\"The base URL for calling the Gemini API.\n            Default value is None.\"\"\",\n        )\n        USE_VERTEX_AI: bool = Field(\n            default=False,\n            description=\"\"\"Whether to use Google Cloud Vertex AI instead of the standard Gemini API.\n            If VERTEX_PROJECT is not set then the plugin will use the Gemini Developer API.\n            Default value is False.\n            Users can opt out of this by setting USE_VERTEX_AI to False in their UserValves.\"\"\",\n        )\n        VERTEX_PROJECT: str | None = Field(\n            default=None,\n            description=\"\"\"The Google Cloud project ID to use with Vertex AI.\n            Default value is None.\"\"\",\n        )\n        VERTEX_LOCATION: str = Field(\n            default=\"global\",\n            description=\"\"\"The Google Cloud region to use with Vertex AI.\n            Default value is 'global'.\"\"\",\n        )\n        MODEL_WHITELIST: str = Field(\n            default=\"*\",\n            description=\"\"\"Comma-separated list of allowed model names.\n            Supports `fnmatch` patterns: *, ?, [seq], [!seq].\n            Default value is * (all models allowed).\"\"\",\n        )\n        MODEL_BLACKLIST: str | None = Field(\n            default=None,\n            description=\"\"\"Comma-separated list of blacklisted model names.\n            Supports `fnmatch` patterns: *, ?, [seq], [!seq].\n            Default value is None (no blacklist).\"\"\",\n        )\n        CACHE_MODELS: bool = Field(\n            default=True,\n            description=\"\"\"Whether to request models only on first load and when white- or blacklist changes.\n            Default value is True.\"\"\",\n        )\n        THINKING_BUDGET: int = Field(\n            default=8192,\n            ge=-1,\n            # The widest possible range is 0 (for Lite/Flash) to 32768 (for Pro).\n            # -1 is used for dynamic thinking budget.\n            # Model-specific constraints are detailed in the description.\n            le=32768,\n            description=\"\"\"Specifies the token budget for the model's internal thinking process,\n            used for complex tasks like tool use. Applicable to Gemini 2.5 models.\n            Default value is 8192. If you want the model to control the thinking budget when using the API, set the thinking budget to -1.\n\n            The valid token range depends on the specific model tier:\n            - **Pro models**: Must be a value between 128 and 32,768.\n            - **Flash and Lite models**: A value between 0 and 24,576. For these\n              models, a value of 0 disables the thinking feature.\n\n            See <https://cloud.google.com/vertex-ai/generative-ai/docs/thinking> for more details.\"\"\",\n        )\n        SHOW_THINKING_SUMMARY: bool = Field(\n            default=True,\n            description=\"\"\"Whether to show the thinking summary in the response.\n            This is only applicable for Gemini 2.5 models.\n            Default value is True.\"\"\",\n        )\n        THINKING_MODEL_PATTERN: str = Field(\n            default=r\"gemini-2.5\",\n            description=\"\"\"Regex pattern to identify thinking models.\n            Default value is r\"gemini-2.5\".\"\"\",\n        )\n        ENABLE_URL_CONTEXT_TOOL: bool = Field(\n            default=False,\n            description=\"\"\"Enable the URL context tool to allow the model to fetch and use content from provided URLs.\n            This tool is only compatible with specific models. Default value is False.\"\"\",\n        )\n        USE_FILES_API: bool = Field(\n            default=True,\n            description=\"\"\"Whether to use the Google Files API for uploading files.\n            This provides caching and performance benefits, but can be disabled for privacy, cost, or compatibility reasons.\n            If disabled, files are sent as raw bytes in the request.\n            Default value is True.\"\"\",\n        )\n        PARSE_YOUTUBE_URLS: bool = Field(\n            default=True,\n            description=\"\"\"Whether to parse YouTube URLs from user messages and provide them as context to the model.\n            If disabled, YouTube links are treated as plain text.\n            This is only applicable for models that support video.\n            Default value is True.\"\"\",\n        )\n        USE_ENTERPRISE_SEARCH: bool = Field(\n            default=False,\n            description=\"\"\"Enable the Enterprise Search tool to allow the model to fetch and use content from provided URLs. \"\"\",\n        )\n        LOG_LEVEL: Literal[\n            \"TRACE\", \"DEBUG\", \"INFO\", \"SUCCESS\", \"WARNING\", \"ERROR\", \"CRITICAL\"\n        ] = Field(\n            default=\"INFO\",\n            description=\"\"\"Select logging level. Use `docker logs -f open-webui` to view logs.\n            Default value is INFO.\"\"\",\n        )\n\n    class UserValves(BaseModel):\n        \"\"\"Defines user-specific settings that can override the default `Valves`.\n\n        The `UserValves` class provides a mechanism for individual users to customize\n        their Gemini API settings for each request. This system is designed as a\n        practical workaround for backend/frontend limitations, enabling per-user\n        configurations.\n\n        Think of the main `Valves` as the global, admin-configured template for the\n        plugin. `UserValves` acts as a user-provided \"overlay\" or \"patch\" that\n        is applied on top of that template at runtime.\n\n        How it works:\n        1.  **Default Behavior:** At the start of a request, the system merges the\n            user's `UserValves` with the admin's `Valves`. If a field in\n            `UserValves` has a value (i.e., is not `None` or an empty string `\"\"`),\n            it overrides the corresponding value from the main `Valves`. If a\n            field is `None` or `\"\"`, the admin's default is used.\n\n        2.  **Special Authentication Logic:** A critical exception exists to enforce\n            security and usage policies. If the admin sets `USER_MUST_PROVIDE_AUTH_CONFIG`\n            to `True` in the main `Valves`, the merging logic changes for any user\n            not on the `AUTH_WHITELIST`:\n            - The user's `GEMINI_API_KEY` is taken directly from their `UserValves`,\n              bypassing the admin's key entirely.\n            - The ability to use the admin-configured Vertex AI is disabled\n              (`USE_VERTEX_AI` is forced to `False`).\n            This ensures that when required, users must use their own credentials\n            and cannot fall back on the shared, system-level authentication.\n\n        This two-tiered configuration allows administrators to set sensible defaults\n        and enforce policies, while still giving users the flexibility to tailor\n        certain parameters, like their API key or model settings, for their own use.\n        \"\"\"\n\n        GEMINI_API_KEY: str | None = Field(\n            default=None,\n            description=\"\"\"Gemini Developer API key.\n            Default value is None (uses the default from Valves, same goes for other options below).\"\"\",\n        )\n        GEMINI_API_BASE_URL: str | None = Field(\n            default=None,\n            description=\"\"\"The base URL for calling the Gemini API\n            Default value is None.\"\"\",\n        )\n        USE_VERTEX_AI: bool | None | Literal[\"\"] = Field(\n            default=None,\n            description=\"\"\"Whether to use Google Cloud Vertex AI instead of the standard Gemini API.\n            Default value is None.\"\"\",\n        )\n        VERTEX_PROJECT: str | None = Field(\n            default=None,\n            description=\"\"\"The Google Cloud project ID to use with Vertex AI.\n            Default value is None.\"\"\",\n        )\n        VERTEX_LOCATION: str | None = Field(\n            default=None,\n            description=\"\"\"The Google Cloud region to use with Vertex AI.\n            Default value is None.\"\"\",\n        )\n        THINKING_BUDGET: int | None | Literal[\"\"] = Field(\n            default=None,\n            description=\"\"\"Specifies the token budget for the model's internal thinking process,\n            used for complex tasks like tool use. Applicable to Gemini 2.5 models.\n            Default value is None. If you want the model to control the thinking budget when using the API, set the thinking budget to -1.\n\n            The valid token range depends on the specific model tier:\n            - **Pro models**: Must be a value between 128 and 32,768.\n            - **Flash and Lite models**: A value between 0 and 24,576. For these\n              models, a value of 0 disables the thinking feature.\n\n            See <https://cloud.google.com/vertex-ai/generative-ai/docs/thinking> for more details.\"\"\",\n        )\n        SHOW_THINKING_SUMMARY: bool | None | Literal[\"\"] = Field(\n            default=None,\n            description=\"\"\"Whether to show the thinking summary in the response.\n            This is only applicable for Gemini 2.5 models.\n            Default value is None.\"\"\",\n        )\n        THINKING_MODEL_PATTERN: str | None = Field(\n            default=None,\n            description=\"\"\"Regex pattern to identify thinking models.\n            Default value is None.\"\"\",\n        )\n        ENABLE_URL_CONTEXT_TOOL: bool | None | Literal[\"\"] = Field(\n            default=None,\n            description=\"\"\"Enable the URL context tool to allow the model to fetch and use content from provided URLs.\n            This tool is only compatible with specific models. Default value is None.\"\"\",\n        )\n        USE_FILES_API: bool | None | Literal[\"\"] = Field(\n            default=None,\n            description=\"\"\"Override the default setting for using the Google Files API.\n            Set to True to force use, False to disable.\n            Default is None (use the admin's setting).\"\"\",\n        )\n        PARSE_YOUTUBE_URLS: bool | None | Literal[\"\"] = Field(\n            default=None,\n            description=\"\"\"Override the default setting for parsing YouTube URLs.\n            Set to True to enable, False to disable.\n            Default is None (use the admin's setting).\"\"\",\n        )\n\n        @field_validator(\"THINKING_BUDGET\", mode=\"after\")\n        @classmethod\n        def validate_thinking_budget_range(cls, v):\n            if v is not None and v != \"\":\n                if not (-1 <= v <= 32768):\n                    raise ValueError(\n                        \"THINKING_BUDGET must be between -1 and 32768, inclusive.\"\n                    )\n            return v\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.file_content_cache = SimpleMemoryCache(serializer=NullSerializer())\n        self.file_id_to_hash_cache = SimpleMemoryCache(serializer=NullSerializer())\n        log.success(\"Function has been initialized.\")\n\n    async def pipes(self) -> list[\"ModelData\"]:\n        \"\"\"Register all available Google models.\"\"\"\n        self._add_log_handler(self.valves.LOG_LEVEL)\n\n        # Clear cache if caching is disabled\n        if not self.valves.CACHE_MODELS:\n            log.debug(\"CACHE_MODELS is False, clearing model cache.\")\n            cache_instance = getattr(self._get_genai_models, \"cache\")\n            await cast(BaseCache, cache_instance).clear()\n\n        log.info(\"Fetching and filtering models from Google API.\")\n        # Get and filter models (potentially cached based on API key, base URL, white- and blacklist)\n        try:\n            client_args = self._prepare_client_args(self.valves)\n            client_args += [self.valves.MODEL_WHITELIST, self.valves.MODEL_BLACKLIST]\n            filtered_models = await self._get_genai_models(*client_args)\n        except GenaiApiError:\n            error_msg = \"Error getting the models from Google API, check the logs.\"\n            return [self._return_error_model(error_msg, exception=True)]\n\n        log.info(f\"Returning {len(filtered_models)} models to Open WebUI.\")\n        log.debug(\"Model list:\", payload=filtered_models, _log_truncation_enabled=False)\n\n        return filtered_models\n\n    async def pipe(\n        self,\n        body: \"Body\",\n        __user__: \"UserData\",\n        __request__: Request,\n        __event_emitter__: Callable[[\"Event\"], Awaitable[None]] | None,\n        __metadata__: \"Metadata\",\n    ) -> AsyncGenerator[dict, None] | str:\n        self._add_log_handler(self.valves.LOG_LEVEL)\n\n        # Apply settings from the user\n        valves: Pipe.Valves = self._get_merged_valves(\n            self.valves, __user__.get(\"valves\"), __user__.get(\"email\")\n        )\n        log.debug(\n            f\"USE_VERTEX_AI: {valves.USE_VERTEX_AI}, VERTEX_PROJECT set: {bool(valves.VERTEX_PROJECT)}, API_KEY set: {bool(valves.GEMINI_API_KEY)}\"\n        )\n\n        log.debug(\n            f\"Getting genai client (potentially cached) for user {__user__['email']}.\"\n        )\n        client = self._get_user_client(valves, __user__[\"email\"])\n\n        if __metadata__.get(\"task\"):\n            log.info(f'{__metadata__[\"task\"]=}, disabling event emissions.')\n            # Task model is not user facing, so we should not emit any events.\n            __event_emitter__ = None\n\n        files_api_manager = FilesAPIManager(\n            client=client,\n            file_cache=self.file_content_cache,\n            id_hash_cache=self.file_id_to_hash_cache,\n            event_emitter=__event_emitter__,\n        )\n\n        # Check if user is chatting with an error model for some reason.\n        if \"error\" in __metadata__[\"model\"][\"id\"]:\n            error_msg = f\"There has been an error during model retrival phase: {str(__metadata__['model'])}\"\n            raise ValueError(error_msg)\n\n        # NOTE: will be \"local\" if Temporary Chat is enabled.\n        chat_id = __metadata__.get(\"chat_id\", \"not_provided\")\n        message_id = __metadata__.get(\"message_id\", \"not_provided\")\n\n        features = __metadata__.get(\"features\", {}) or {}\n        log.info(\n            \"Converting Open WebUI's `body` dict into list of `Content` objects that `google-genai` understands.\"\n        )\n        # URL context front-end button takes precedence over valves setting if it is enabled.\n        if self._is_function_active(\"gemini_url_context_toggle\"):\n            valves.ENABLE_URL_CONTEXT_TOOL = features.get(\"url_context\", False)\n            log.info(\n                \"URL context toggle filter is active. \"\n                f\"Setting valves.ENABLE_URL_CONTEXT_TOOL to {valves.ENABLE_URL_CONTEXT_TOOL}.\"\n            )\n        else:\n            log.warning(\n                \"Gemini URL Context Toggle filter is not active. \"\n                \"Install or enable it if you want to toggle URL context tool on/off through a front-end button.\"\n            )\n\n        builder = GeminiContentBuilder(\n            messages_body=body.get(\"messages\"),\n            metadata_body=__metadata__,\n            user_data=__user__,\n            event_emitter=__event_emitter__,\n            valves=valves,\n            files_api_manager=files_api_manager,\n        )\n        contents = await builder.build_contents()\n\n        # Assemble GenerateContentConfig\n        safety_settings: list[types.SafetySetting] | None = __metadata__.get(\n            \"safety_settings\"\n        )\n        model_name = re.sub(r\"^.*?[./]\", \"\", body.get(\"model\", \"\"))\n\n        thinking_conf = None\n        if re.search(self.valves.THINKING_MODEL_PATTERN, model_name, re.IGNORECASE):\n            log.info(f\"Model ID '{model_name}' allows adjusting the thinking settings.\")\n            thinking_conf = types.ThinkingConfig(\n                thinking_budget=valves.THINKING_BUDGET,\n                include_thoughts=valves.SHOW_THINKING_SUMMARY,\n            )\n\n        if self._is_function_active(\"gemini_reasoning_toggle\"):\n            # NOTE: Gemini 2.5 Pro supports reasoning budget but not toggling reasoning on/off.\n            if re.search(\n                r\"gemini-2.5-(flash|lite)\", model_name, re.IGNORECASE\n            ) and not features.get(\"reason\"):\n                log.info(\n                    f\"Model ID '{model_name}' allows turning off the reasoning feature. \"\n                    \"Reasoning is currently toggled off in the UI. Setting thinking budget to 0.\"\n                )\n                thinking_conf = types.ThinkingConfig(\n                    thinking_budget=0,\n                    include_thoughts=valves.SHOW_THINKING_SUMMARY,\n                )\n        else:\n            log.warning(\n                \"Gemini Reasoning Toggle filter is not active. \"\n                \"Install or enable it if you want to toggle Gemini 2.5 Flash or Lite reasoning on/off through a front-end button.\"\n            )\n        # TODO: Take defaults from the general front-end config.\n        gen_content_conf = types.GenerateContentConfig(\n            system_instruction=builder.system_prompt,\n            temperature=body.get(\"temperature\"),\n            top_p=body.get(\"top_p\"),\n            top_k=body.get(\"top_k\"),\n            max_output_tokens=body.get(\"max_tokens\"),\n            stop_sequences=body.get(\"stop\"),\n            safety_settings=safety_settings,\n            thinking_config=thinking_conf,\n        )\n        gen_content_conf.response_modalities = [\"Text\"]\n        if (\n            \"gemini-2.0-flash-preview-image-generation\" in model_name\n            or \"gemma\" in model_name\n        ):\n            if \"gemini-2.0-flash-preview-image-generation\" in model_name:\n                gen_content_conf.response_modalities.append(\"Image\")\n            # TODO: append to user message instead.\n            if gen_content_conf.system_instruction:\n                gen_content_conf.system_instruction = None\n                log.warning(\n                    \"Image Generation model does not support the system prompt message! Removing the system prompt.\"\n                )\n        gen_content_conf.tools = []\n\n        if features.get(\"google_search_tool\"):\n            log.info(\"Using grounding with Google Search as a Tool.\")\n            if valves.USE_ENTERPRISE_SEARCH and client.vertexai:\n                log.info(\"Using Enterprise Web Search instead of Google Search.\")\n                gen_content_conf.tools.append(\n                    types.Tool(enterprise_web_search=types.EnterpriseWebSearch())\n                )\n            else:\n                gen_content_conf.tools.append(\n                    types.Tool(google_search=types.GoogleSearch())\n                )\n        elif features.get(\"google_search_retrieval\"):\n            log.info(\"Using grounding with Google Search Retrieval.\")\n            gs = types.GoogleSearchRetrieval(\n                dynamic_retrieval_config=types.DynamicRetrievalConfig(\n                    dynamic_threshold=features.get(\"google_search_retrieval_threshold\")\n                )\n            )\n            gen_content_conf.tools.append(types.Tool(google_search_retrieval=gs))\n        # NB: It is not possible to use both Search and Code execution at the same time,\n        # however, it can be changed later, so let's just handle it as a common error\n        if features.get(\"google_code_execution\"):\n            log.info(\"Using code execution on Google side.\")\n            gen_content_conf.tools.append(\n                types.Tool(code_execution=types.ToolCodeExecution())\n            )\n        gen_content_args = {\n            \"model\": model_name,\n            \"contents\": contents,\n            \"config\": gen_content_conf,\n        }\n        log.debug(\"Passing these args to the Google API:\", payload=gen_content_args)\n\n        # Add URL context tool if enabled and model is compatible\n        if valves.ENABLE_URL_CONTEXT_TOOL:\n            compatible_models_for_url_context = [\n                \"gemini-2.5-pro\",\n                \"gemini-2.5-flash\",\n                \"gemini-2.5-flash-lite\",\n                \"gemini-2.5-flash-lite-preview-06-17\",\n                \"gemini-2.5-pro-preview-06-05\",\n                \"gemini-2.5-pro-preview-05-06\",\n                \"gemini-2.5-flash-preview-05-20\",\n                \"gemini-2.0-flash\",\n                \"gemini-2.0-flash-001\",\n                \"gemini-2.0-flash-live-001\",\n            ]\n            if model_name in compatible_models_for_url_context:\n                if client.vertexai and (len(gen_content_conf.tools) > 0):\n                    log.warning(\n                        \"URL context tool is enabled, but Vertex AI is used with multiple tools. Skipping.\"\n                    )\n                else:\n                    log.info(\n                        f\"Model {model_name} is compatible with URL context tool. Enabling.\"\n                    )\n                    gen_content_conf.tools.append(\n                        types.Tool(url_context=types.UrlContext())\n                    )\n            else:\n                log.warning(\n                    f\"URL context tool is enabled, but model {model_name} is not in the compatible list. Skipping.\"\n                )\n        if body.get(\"stream\", False):\n            # Streaming response\n\n            asyncio.create_task(\n                emit_status(\n                    \"Waiting for first token from Google...\",\n                    __event_emitter__,\n                    done=False,\n                )\n            )\n            response_stream: AsyncIterator[types.GenerateContentResponse] = (\n                await client.aio.models.generate_content_stream(**gen_content_args)  # type: ignore\n            )\n\n            log.info(\"Streaming enabled. Returning AsyncGenerator.\")\n            return self._stream_response_generator(\n                response_stream,\n                __request__,\n                model_name,\n                __event_emitter__,\n                __user__[\"id\"],\n                chat_id,\n                message_id,\n            )\n        else:\n            # Non-streaming response.\n            if \"gemini-2.0-flash-preview-image-generation\" in model_name:\n                warn_msg = \"Non-streaming responses with native image gen are not currently supported! Stay tuned! Please enable streaming.\"\n                raise NotImplementedError(warn_msg)\n            # TODO: Support native image gen here too.\n            # TODO: Support code execution here too.\n            asyncio.create_task(\n                emit_status(\n                    \"Waiting for response from Google...\",\n                    __event_emitter__,\n                    done=False,\n                )\n            )\n            try:\n                # TODO: Support native image gen here too.\n                # TODO: Support code execution here too.\n                res = await client.aio.models.generate_content(**gen_content_args)\n            finally:\n                asyncio.create_task(\n                    emit_status(\n                        \"Response received\",\n                        __event_emitter__,\n                        done=True,\n                        hidden=True,\n                    )\n                )\n            if raw_text := res.text:\n                log.info(\"Non-streaming response finished successfully!\")\n                log.debug(\"Non-streaming response:\", payload=res)\n                await self._do_post_processing(\n                    res, __event_emitter__, __request__, chat_id, message_id\n                )\n                log.info(\n                    \"Streaming disabled. Returning full response as str. \"\n                    \"With that Pipe.pipe method has finished it's run!\"\n                )\n                return raw_text\n            else:\n                warn_msg = \"Non-streaming response did not have any text inside it.\"\n                raise ValueError(warn_msg)\n\n    # region 2. Helper methods inside the Pipe class\n\n    # region 2.1 Client initialization\n    @staticmethod\n    @cache\n    def _get_or_create_genai_client(\n        api_key: str | None = None,\n        base_url: str | None = None,\n        use_vertex_ai: bool | None = None,\n        vertex_project: str | None = None,\n        vertex_location: str | None = None,\n    ) -> genai.Client:\n        \"\"\"\n        Creates a genai.Client instance or retrieves it from cache.\n        Raises GenaiApiError on failure.\n        \"\"\"\n\n        if not vertex_project and not api_key:\n            # FIXME: More detailed reason in the exception (tell user to set the API key).\n            msg = \"Neither VERTEX_PROJECT nor GEMINI_API_KEY is set.\"\n            raise GenaiApiError(msg)\n\n        if use_vertex_ai and vertex_project:\n            kwargs = {\n                \"vertexai\": True,\n                \"project\": vertex_project,\n                \"location\": vertex_location,\n            }\n            api = \"Vertex AI\"\n        else:  # Covers (use_vertex_ai and not vertex_project) OR (not use_vertex_ai)\n            if use_vertex_ai and not vertex_project:\n                log.warning(\n                    \"Vertex AI is enabled but no project is set. \"\n                    \"Using Gemini Developer API.\"\n                )\n            # This also implicitly covers the case where api_key might be None,\n            # which is handled by the initial check or the SDK.\n            kwargs = {\n                \"api_key\": api_key,\n                \"http_options\": types.HttpOptions(base_url=base_url),\n            }\n            api = \"Gemini Developer API\"\n\n        try:\n            client = genai.Client(**kwargs)\n            log.success(f\"{api} Genai client successfully initialized.\")\n            return client\n        except Exception as e:\n            raise GenaiApiError(f\"{api} Genai client initialization failed: {e}\") from e\n\n    def _get_user_client(self, valves: \"Pipe.Valves\", user_email: str) -> genai.Client:\n        user_whitelist = (\n            valves.AUTH_WHITELIST.split(\",\") if valves.AUTH_WHITELIST else []\n        )\n        log.debug(\n            f\"User whitelist: {user_whitelist}, user email: {user_email}, \"\n            f\"USER_MUST_PROVIDE_AUTH_CONFIG: {valves.USER_MUST_PROVIDE_AUTH_CONFIG}\"\n        )\n        if valves.USER_MUST_PROVIDE_AUTH_CONFIG and user_email not in user_whitelist:\n            if not valves.GEMINI_API_KEY:\n                error_msg = (\n                    \"User must provide their own authentication configuration. \"\n                    \"Please set GEMINI_API_KEY in your UserValves.\"\n                )\n                raise ValueError(error_msg)\n        try:\n            client_args = self._prepare_client_args(valves)\n            client = self._get_or_create_genai_client(*client_args)\n        except GenaiApiError as e:\n            error_msg = f\"Failed to initialize genai client for user {user_email}: {e}\"\n            # FIXME: include correct traceback.\n            raise ValueError(error_msg) from e\n        return client\n\n    @staticmethod\n    def _prepare_client_args(\n        source_valves: \"Pipe.Valves | Pipe.UserValves\",\n    ) -> list[str | bool | None]:\n        \"\"\"Prepares arguments for _get_or_create_genai_client from source_valves.\"\"\"\n        ATTRS = [\n            \"GEMINI_API_KEY\",\n            \"GEMINI_API_BASE_URL\",\n            \"USE_VERTEX_AI\",\n            \"VERTEX_PROJECT\",\n            \"VERTEX_LOCATION\",\n        ]\n        return [getattr(source_valves, attr) for attr in ATTRS]\n\n    # endregion 2.1 Client initialization\n\n    # region 2.2 Model retrival from Google API\n    @cached()  # aiocache.cached for async method\n    async def _get_genai_models(\n        self,\n        api_key: str | None,\n        base_url: str | None,\n        use_vertex_ai: bool | None,  # User's preference from config\n        vertex_project: str | None,\n        vertex_location: str | None,\n        whitelist_str: str,\n        blacklist_str: str | None,\n    ) -> list[\"ModelData\"]:\n        \"\"\"\n        Gets valid Google models from API(s) and filters them.\n        If use_vertex_ai, vertex_project, and api_key are all provided,\n        models are fetched from both Vertex AI and Gemini Developer API and merged.\n        \"\"\"\n        all_raw_models: list[types.Model] = []\n\n        # Condition for fetching from both sources\n        fetch_both = bool(use_vertex_ai and vertex_project and api_key)\n\n        if fetch_both:\n            log.info(\n                \"Attempting to fetch models from both Gemini Developer API and Vertex AI.\"\n            )\n            gemini_models_list: list[types.Model] = []\n            vertex_models_list: list[types.Model] = []\n\n            # TODO: perf, consider parallelizing these two fetches\n            # 1. Fetch from Gemini Developer API\n            try:\n                gemini_client = self._get_or_create_genai_client(\n                    api_key=api_key,\n                    base_url=base_url,\n                    use_vertex_ai=False,  # Explicitly target Gemini API\n                    vertex_project=None,\n                    vertex_location=None,\n                )\n                gemini_models_list = await self._fetch_models_from_client_internal(\n                    gemini_client, \"Gemini Developer API\"\n                )\n            except GenaiApiError as e:\n                log.warning(\n                    f\"Failed to initialize or retrieve models from Gemini Developer API: {e}\"\n                )\n            except Exception as e:\n                log.warning(\n                    f\"An unexpected error occurred with Gemini Developer API models: {e}\",\n                    exc_info=True,\n                )\n\n            # 2. Fetch from Vertex AI\n            try:\n                vertex_client = self._get_or_create_genai_client(\n                    use_vertex_ai=True,  # Explicitly target Vertex AI\n                    vertex_project=vertex_project,\n                    vertex_location=vertex_location,\n                    api_key=None,  # API key is not used for Vertex AI with project auth\n                    base_url=base_url,  # Pass base_url for potential Vertex custom endpoints\n                )\n                vertex_models_list = await self._fetch_models_from_client_internal(\n                    vertex_client, \"Vertex AI\"\n                )\n            except GenaiApiError as e:\n                log.warning(\n                    f\"Failed to initialize or retrieve models from Vertex AI: {e}\"\n                )\n            except Exception as e:\n                log.warning(\n                    f\"An unexpected error occurred with Vertex AI models: {e}\",\n                    exc_info=True,\n                )\n\n            # 3. Combine and de-duplicate\n            # Prioritize models from Gemini Developer API in case of ID collision\n            combined_models_dict: dict[str, types.Model] = {}\n\n            for model in gemini_models_list:\n                if model.name:\n                    model_id = Pipe.strip_prefix(model.name)\n                    if model_id and model_id not in combined_models_dict:\n                        combined_models_dict[model_id] = model\n                else:\n                    log.trace(\n                        f\"Gemini model without a name encountered: {model.display_name or 'N/A'}\"\n                    )\n\n            for model in vertex_models_list:\n                if model.name:\n                    model_id = Pipe.strip_prefix(model.name)\n                    if model_id:\n                        if model_id not in combined_models_dict:\n                            combined_models_dict[model_id] = model\n                        else:\n                            log.info(\n                                f\"Duplicate model ID '{model_id}' from Vertex AI already sourced from Gemini API. Keeping Gemini API version.\"\n                            )\n                else:\n                    log.trace(\n                        f\"Vertex AI model without a name encountered: {model.display_name or 'N/A'}\"\n                    )\n\n            all_raw_models = list(combined_models_dict.values())\n\n            log.info(\n                f\"Fetched {len(gemini_models_list)} models from Gemini API, \"\n                f\"{len(vertex_models_list)} from Vertex AI. \"\n                f\"Combined to {len(all_raw_models)} unique models.\"\n            )\n\n            if not all_raw_models and (gemini_models_list or vertex_models_list):\n                log.warning(\n                    \"Models were fetched but resulted in an empty list after de-duplication, possibly due to missing names or empty/duplicate IDs.\"\n                )\n\n            if not all_raw_models and not gemini_models_list and not vertex_models_list:\n                raise GenaiApiError(\n                    \"Failed to retrieve models: Both Gemini Developer API and Vertex AI attempts yielded no models.\"\n                )\n\n        else:  # Single source logic\n            # Determine if we are effectively using Vertex AI or Gemini API\n            # This depends on user's config (use_vertex_ai) and availability of project/key\n            client_target_is_vertex = bool(use_vertex_ai and vertex_project)\n            client_source_name = (\n                \"Vertex AI\" if client_target_is_vertex else \"Gemini Developer API\"\n            )\n            log.info(\n                f\"Attempting to fetch models from a single source: {client_source_name}.\"\n            )\n\n            try:\n                client = self._get_or_create_genai_client(\n                    api_key=api_key,\n                    base_url=base_url,\n                    use_vertex_ai=client_target_is_vertex,  # Pass the determined target\n                    vertex_project=vertex_project if client_target_is_vertex else None,\n                    vertex_location=(\n                        vertex_location if client_target_is_vertex else None\n                    ),\n                )\n                all_raw_models = await self._fetch_models_from_client_internal(\n                    client, client_source_name\n                )\n\n                if not all_raw_models:\n                    raise GenaiApiError(\n                        f\"No models retrieved from {client_source_name}. This could be due to an API error, network issue, or no models being available.\"\n                    )\n\n            except GenaiApiError as e:\n                raise GenaiApiError(\n                    f\"Failed to get models from {client_source_name}: {e}\"\n                ) from e\n            except Exception as e:\n                log.error(\n                    f\"An unexpected error occurred while configuring client or fetching models from {client_source_name}: {e}\",\n                    exc_info=True,\n                )\n                raise GenaiApiError(\n                    f\"An unexpected error occurred while retrieving models from {client_source_name}: {e}\"\n                ) from e\n\n        # --- Common processing for all_raw_models ---\n\n        if not all_raw_models:\n            log.warning(\"No models available after attempting all configured sources.\")\n            return []\n\n        log.info(f\"Processing {len(all_raw_models)} unique raw models.\")\n\n        generative_models: list[types.Model] = []\n        for model in all_raw_models:\n            if model.name is None:\n                log.trace(\n                    f\"Skipping model with no name during generative filter: {model.display_name or 'N/A'}\"\n                )\n                continue\n            actions = model.supported_actions\n            if (\n                actions is None or \"generateContent\" in actions\n            ):  # Includes models if actions is None (e.g., Vertex)\n                generative_models.append(model)\n            else:\n                log.trace(\n                    f\"Model '{model.name}' (ID: {Pipe.strip_prefix(model.name)}) skipped, not generative (actions: {actions}).\"\n                )\n\n        if not generative_models:\n            log.warning(\n                \"No generative models found after filtering all retrieved models.\"\n            )\n            return []\n\n        def match_patterns(\n            name_to_check: str, list_of_patterns_str: str | None\n        ) -> bool:\n            if not list_of_patterns_str:\n                return False\n            patterns = [\n                pat for pat in list_of_patterns_str.replace(\" \", \"\").split(\",\") if pat\n            ]  # Ensure pat is not empty\n            return any(fnmatch.fnmatch(name_to_check, pat) for pat in patterns)\n\n        filtered_models_data: list[\"ModelData\"] = []\n        for model in generative_models:\n            # model.name is guaranteed non-None by generative_models filter logic\n            stripped_name = Pipe.strip_prefix(model.name)  # type: ignore\n\n            if not stripped_name:\n                log.warning(\n                    f\"Model '{model.name}' (display: {model.display_name}) resulted in an empty ID after stripping. Skipping.\"\n                )\n                continue\n\n            passes_whitelist = not whitelist_str or match_patterns(\n                stripped_name, whitelist_str\n            )\n            passes_blacklist = not blacklist_str or not match_patterns(\n                stripped_name, blacklist_str\n            )\n\n            if passes_whitelist and passes_blacklist:\n                filtered_models_data.append(\n                    {\n                        \"id\": stripped_name,\n                        \"name\": model.display_name or stripped_name,\n                        \"description\": model.description,\n                    }\n                )\n            else:\n                log.trace(\n                    f\"Model ID '{stripped_name}' filtered out by whitelist/blacklist. Whitelist match: {passes_whitelist}, Blacklist pass: {passes_blacklist}\"\n                )\n\n        log.info(\n            f\"Filtered {len(generative_models)} generative models down to {len(filtered_models_data)} models based on white/blacklists.\"\n        )\n        return filtered_models_data\n\n    # TODO: Use cache for this method too?\n    async def _fetch_models_from_client_internal(\n        self, client: genai.Client, source_name: str\n    ) -> list[types.Model]:\n        \"\"\"Helper to fetch models from a given client and handle common exceptions.\"\"\"\n        try:\n            google_models_pager = await client.aio.models.list(\n                config={\"query_base\": True}  # Fetch base models by default\n            )\n            models = [model async for model in google_models_pager]\n            log.info(f\"Retrieved {len(models)} models from {source_name}.\")\n            log.trace(\n                f\"All models returned by {source_name}:\", payload=models\n            )  # Can be verbose\n            return models\n        except Exception as e:\n            log.error(f\"Retrieving models from {source_name} failed: {e}\")\n            # Return empty list; caller decides if this is fatal for the whole operation.\n            return []\n\n    @staticmethod\n    def _return_error_model(\n        error_msg: str, warning: bool = False, exception: bool = True\n    ) -> \"ModelData\":\n        \"\"\"Returns a placeholder model for communicating error inside the pipes method to the front-end.\"\"\"\n        if warning:\n            log.opt(depth=1, exception=False).warning(error_msg)\n        else:\n            log.opt(depth=1, exception=exception).error(error_msg)\n        return {\n            \"id\": \"error\",\n            \"name\": \"[gemini_manifold] \" + error_msg,\n            \"description\": error_msg,\n        }\n\n    @staticmethod\n    def strip_prefix(model_name: str) -> str:\n        \"\"\"\n        Extract the model identifier using regex, handling various naming conventions.\n        e.g., \"gemini_manifold_google_genai.gemini-2.5-flash-preview-04-17\" -> \"gemini-2.5-flash-preview-04-17\"\n        e.g., \"models/gemini-1.5-flash-001\" -> \"gemini-1.5-flash-001\"\n        e.g., \"publishers/google/models/gemini-1.5-pro\" -> \"gemini-1.5-pro\"\n        \"\"\"\n        # Use regex to remove everything up to and including the last '/' or the first '.'\n        stripped = re.sub(r\"^(?:.*/|[^.]*\\.)\", \"\", model_name)\n        return stripped\n\n    # endregion 2.2 Model retrival from Google API\n\n    # region 2.3 Model response streaming\n    async def _stream_response_generator(\n        self,\n        response_stream: AsyncIterator[types.GenerateContentResponse],\n        __request__: Request,\n        model: str,\n        event_emitter: Callable[[\"Event\"], Awaitable[None]] | None,\n        user_id: str,\n        chat_id: str,\n        message_id: str,\n    ) -> AsyncGenerator[dict, None]:\n        \"\"\"\n        Yields structured dictionary chunks from the stream, counts tag substitutions\n        for a final toast notification, and handles post-processing.\n        \"\"\"\n        final_response_chunk: types.GenerateContentResponse | None = None\n        error_occurred = False\n        total_substitutions = 0\n\n        try:\n            part_processor = self._process_parts_to_structured_stream(\n                response_stream,\n                __request__,\n                model,\n                user_id,\n                chat_id,\n                message_id,\n                event_emitter,\n            )\n            async for structured_chunk, count, raw_chunk in part_processor:\n                if count > 0:\n                    total_substitutions += count\n                    log.debug(f\"Disabled {count} special tag(s) in a chunk.\")\n\n                if raw_chunk:\n                    final_response_chunk = raw_chunk\n                yield structured_chunk\n\n        except Exception as e:\n            error_occurred = True\n            error_msg = f\"Stream ended with error: {e}\"\n            # Using just raise does not work correctly for some reason, so we use a custom reverse engineered emit_error function for that.\n            await emit_error(error_msg, event_emitter)\n            # FIXME: Ensure last emitted status message gets hidden.\n        finally:\n            if total_substitutions > 0 and not error_occurred:\n                plural_s = \"s\" if total_substitutions > 1 else \"\"\n                toast_msg = (\n                    f\"For clarity, {total_substitutions} special tag{plural_s} \"\n                    \"were disabled in the response by injecting a zero-width space (ZWS).\"\n                )\n                await emit_toast(toast_msg, event_emitter, \"info\")\n\n            if not error_occurred:\n                log.info(\"Stream finished successfully!\")\n                log.debug(\"Last chunk:\", payload=final_response_chunk)\n\n            try:\n                await self._do_post_processing(\n                    final_response_chunk,\n                    event_emitter,\n                    __request__,\n                    chat_id=chat_id,\n                    message_id=message_id,\n                    stream_error_happened=error_occurred,\n                )\n            except Exception as e:\n                error_msg = f\"Post-processing failed with error:\\n\\n{e}\"\n                await emit_toast(error_msg, event_emitter, \"error\")\n                log.exception(error_msg)\n\n            log.debug(\"AsyncGenerator finished.\")\n\n    async def _process_parts_to_structured_stream(\n        self,\n        response_stream: AsyncIterator[types.GenerateContentResponse],\n        __request__: Request,\n        model: str,\n        user_id: str,\n        chat_id: str,\n        message_id: str,\n        event_emitter: Callable[[\"Event\"], Awaitable[None]] | None,\n    ) -> AsyncGenerator[tuple[dict, int, types.GenerateContentResponse | None], None]:\n        \"\"\"\n        Processes a stream of Gemini responses, yielding structured dictionaries,\n        a substitution count for the ZWS safeguard, and the raw chunk.\n        \"\"\"\n        first_chunk_received = False\n        try:\n            async for chunk in response_stream:\n                if not first_chunk_received:\n                    # This is the first chunk. End the waiting status.\n                    asyncio.create_task(\n                        emit_status(\n                            \"First token received\",\n                            event_emitter,\n                            done=True,\n                            hidden=True,\n                        )\n                    )\n                    first_chunk_received = True\n\n                if not (candidate := self._get_first_candidate(chunk.candidates)):\n                    log.warning(\"Stream chunk has no candidates, skipping.\")\n                    continue\n                if not (parts := candidate.content and candidate.content.parts):\n                    log.warning(\"Candidate has no content parts, skipping.\")\n                    continue\n\n                for part in parts:\n                    # Initialize variables at the start of each loop to satisfy the linter\n                    # and ensure they always have a defined state.\n                    payload: dict[str, str] | None = None\n                    count: int = 0\n                    key: str = \"content\"\n\n                    match part:\n                        case types.Part(text=str(text), thought=True):\n                            # It's a thought, so we'll use the \"reasoning\" key.\n                            key = \"reasoning\"\n                            sanitized_text, count = self._disable_special_tags(text)\n                            payload = {key: sanitized_text}\n                        case types.Part(text=str(text)):\n                            # It's regular content, using the default \"content\" key.\n                            sanitized_text, count = self._disable_special_tags(text)\n                            payload = {key: sanitized_text}\n                        case types.Part(inline_data=data):\n                            if not data:\n                                log.warning(\n                                    \"Model response stream Part has an inline_data field but it is empty, skipping.\"\n                                )\n                                continue\n                            # Image parts don't need tag disabling.\n                            processed_text = await self._process_image_part(\n                                data, model, user_id, chat_id, message_id, __request__\n                            )\n                            payload = {\"content\": processed_text}\n                        case types.Part(executable_code=code):\n                            processed_text = self._process_executable_code_part(code)\n                            # Code blocks are already formatted and safe.\n                            if processed_text:\n                                payload = {\"content\": processed_text}\n                        case types.Part(code_execution_result=result):\n                            processed_text = self._process_code_execution_result_part(\n                                result\n                            )\n                            # Code results are also safe.\n                            if processed_text:\n                                payload = {\"content\": processed_text}\n\n                    if payload:\n                        structured_chunk = {\"choices\": [{\"delta\": payload}]}\n                        yield structured_chunk, count, chunk\n        except Exception:\n            raise\n        finally:\n            if not first_chunk_received:\n                # Emit done status if error occurs before the first chunk.\n                await emit_status(\n                    \"Error occurred before receiving the first token from Google.\",\n                    event_emitter,\n                    done=True,\n                    hidden=True,\n                )\n\n    @staticmethod\n    def _disable_special_tags(text: str) -> tuple[str, int]:\n        \"\"\"\n        Finds special tags in a text chunk and inserts a Zero-Width Space (ZWS)\n        to prevent them from being parsed by the Open WebUI backend's legacy system.\n        This is a safeguard against accidental tag generation by the model.\n        \"\"\"\n        if not text:\n            return \"\", 0\n\n        # The regex finds '<' followed by an optional '/' and then one of the special tags.\n        # The inner parentheses group the tags, so the optional '/' applies to all of them.\n        TAG_REGEX = re.compile(\n            r\"<(/?\"\n            + \"(\"\n            + \"|\".join(re.escape(tag) for tag in SPECIAL_TAGS_TO_DISABLE)\n            + \")\"\n            + r\")\"\n        )\n        # The substitution injects a ZWS, e.g., '</think>' becomes '<ZWS/think'.\n        modified_text, num_substitutions = TAG_REGEX.subn(rf\"<{ZWS}\\1\", text)\n        return modified_text, num_substitutions\n\n    async def _process_image_part(\n        self,\n        inline_data: types.Blob,\n        model: str,\n        user_id: str,\n        chat_id: str,\n        message_id: str,\n        request: Request,\n    ) -> str:\n        \"\"\"\n        Handles image data by saving it to the Open WebUI backend and returning a markdown link.\n        \"\"\"\n        mime_type = inline_data.mime_type\n        image_data = inline_data.data\n\n        if mime_type and image_data:\n            image_url = await self._upload_image(\n                image_data=image_data,\n                mime_type=mime_type,\n                model=model,\n                user_id=user_id,\n                chat_id=chat_id,\n                message_id=message_id,\n                __request__=request,\n            )\n        else:\n            log.warning(\n                \"Image part has no mime_type or data, cannot upload image. \"\n                \"Returning a placeholder message.\"\n            )\n            image_url = None\n\n        return (\n            f\"![Generated Image]({image_url})\"\n            if image_url\n            else \"*An error occurred while trying to store this model generated image.*\"\n        )\n\n    async def _upload_image(\n        self,\n        image_data: bytes,\n        mime_type: str,\n        model: str,\n        user_id: str,\n        chat_id: str,\n        message_id: str,\n        __request__: Request,\n    ) -> str | None:\n        \"\"\"\n        Helper method that uploads a generated image to the configured Open WebUI storage provider.\n        Returns the url to the uploaded image.\n        \"\"\"\n        image_format = mimetypes.guess_extension(mime_type) or \".png\"\n        id = str(uuid.uuid4())\n        name = f\"generated-image{image_format}\"\n\n        # The final filename includes the unique ID to prevent collisions.\n        imagename = f\"{id}_{name}\"\n        image = io.BytesIO(image_data)\n\n        # Create a clean, precise metadata object linking to the generation context.\n        image_metadata = {\n            \"model\": model,\n            \"chat_id\": chat_id,\n            \"message_id\": message_id,\n        }\n\n        log.info(\"Uploading the model-generated image to the Open WebUI backend.\")\n\n        try:\n            contents, image_path = await asyncio.to_thread(\n                Storage.upload_file, image, imagename, tags={}\n            )\n        except Exception:\n            log.exception(\"Error occurred during upload to the storage provider.\")\n            return None\n\n        log.debug(\"Adding the image file to the Open WebUI files database.\")\n        file_item = await asyncio.to_thread(\n            Files.insert_new_file,\n            user_id,\n            FileForm(\n                id=id,\n                filename=name,\n                path=image_path,\n                meta={\n                    \"name\": name,\n                    \"content_type\": mime_type,\n                    \"size\": len(contents),\n                    \"data\": image_metadata,\n                },\n            ),\n        )\n        if not file_item:\n            log.warning(\"Image upload to Open WebUI database likely failed.\")\n            return None\n\n        image_url: str = __request__.app.url_path_for(\n            \"get_file_content_by_id\", id=file_item.id\n        )\n        log.success(\"Image upload finished!\")\n        return image_url\n\n    def _process_executable_code_part(\n        self, executable_code_part: types.ExecutableCode | None\n    ) -> str | None:\n        \"\"\"\n        Processes an executable code part and returns the formatted string representation.\n        \"\"\"\n\n        if not executable_code_part:\n            return None\n\n        lang_name = \"python\"  # Default language\n        if executable_code_part_lang_enum := executable_code_part.language:\n            if lang_name := executable_code_part_lang_enum.name:\n                lang_name = executable_code_part_lang_enum.name.lower()\n            else:\n                log.warning(\n                    f\"Could not extract language name from {executable_code_part_lang_enum}. Default to python.\"\n                )\n        else:\n            log.warning(\"Language Enum is None, defaulting to python.\")\n\n        if executable_code_part_code := executable_code_part.code:\n            return f\"```{lang_name}\\n{executable_code_part_code.rstrip()}\\n```\\n\\n\"\n        return \"\"\n\n    def _process_code_execution_result_part(\n        self, code_execution_result_part: types.CodeExecutionResult | None\n    ) -> str | None:\n        \"\"\"\n        Processes a code execution result part and returns the formatted string representation.\n        \"\"\"\n\n        if not code_execution_result_part:\n            return None\n\n        if code_execution_result_part_output := code_execution_result_part.output:\n            return f\"**Output:**\\n\\n```\\n{code_execution_result_part_output.rstrip()}\\n```\\n\\n\"\n        else:\n            return None\n\n    # endregion 2.3 Model response streaming\n\n    # region 2.4 Post-processing\n    async def _do_post_processing(\n        self,\n        model_response: types.GenerateContentResponse | None,\n        event_emitter: Callable[[\"Event\"], Awaitable[None]] | None,\n        request: Request,\n        chat_id: str,\n        message_id: str,\n        *,\n        stream_error_happened: bool = False,\n    ):\n        \"\"\"Handles emitting usage, grounding, and sources after the main response/stream is done.\"\"\"\n        log.info(\"Post-processing the model response.\")\n        if stream_error_happened:\n            log.warning(\n                \"An error occured during the stream, cannot do post-processing.\"\n            )\n            # All the needed metadata is always in the last chunk, so if an error happened\n            # during the stream, we likely don't have the final response object.\n            return\n        if not model_response:\n            log.warning(\"model_response is empty, cannot do post-processing.\")\n            return\n        if not (candidate := self._get_first_candidate(model_response.candidates)):\n            log.warning(\n                \"Response does not contain any canditates. Cannot do post-processing.\"\n            )\n            return\n\n        finish_reason = candidate.finish_reason\n        finish_message = candidate.finish_message\n\n        # Bucketize finish reasons to determine logging and notification levels.\n        NORMAL_REASONS = {types.FinishReason.STOP, types.FinishReason.MAX_TOKENS}\n        WARNING_REASONS = {types.FinishReason.RECITATION, types.FinishReason.OTHER}\n\n        if finish_reason in NORMAL_REASONS:\n            log.debug(f\"Stream finished normally with reason: {finish_reason.name}.\")  # type: ignore\n        elif finish_reason is None or finish_reason in WARNING_REASONS:\n            reason_str = (\n                \"an unknown reason\" if finish_reason is None else finish_reason.name\n            )\n            msg = f\"Stream finished with a warning: {reason_str}.\"\n            if finish_message:\n                msg += f\"\\n\\nDetails: {finish_message}\"\n            log.warning(msg)\n            await emit_toast(msg, event_emitter, \"warning\")\n        else:  # All other reasons are treated as errors.\n            reason_str = finish_reason.name\n            msg = f\"Stream finished with an error: {reason_str}.\"\n            if finish_message:\n                msg += f\"\\n\\nDetails: {finish_message}\"\n            log.error(msg)\n            await emit_toast(msg, event_emitter, \"error\")\n\n        # TODO: Emit a toast message if url context retrieval was not successful.\n\n        # Attempt to emit token usage data even if the finish reason was problematic,\n        # as usage data might still be available.\n        if usage_event := self._get_usage_data_event(model_response):\n            if event_emitter:\n                log.debug(\"Emitting usage data:\", payload=usage_event)\n                await event_emitter(usage_event)\n\n        self._add_grounding_data_to_state(model_response, request, chat_id, message_id)\n\n    def _add_grounding_data_to_state(\n        self,\n        response: types.GenerateContentResponse,\n        request: Request,\n        chat_id: str,\n        message_id: str,\n    ):\n        candidate = self._get_first_candidate(response.candidates)\n        grounding_metadata_obj = candidate.grounding_metadata if candidate else None\n\n        storage_key = f\"grounding_{chat_id}_{message_id}\"\n\n        if grounding_metadata_obj:\n            log.debug(\n                f\"Found grounding metadata. Storing in in request's app state using key {storage_key}.\"\n            )\n            # Using shared `request.app.state` to pass grounding metadata to Filter.outlet.\n            # This is necessary because the Pipe finishes during the initial `/api/completion` request,\n            # while Filter.outlet is invoked by a separate, later `/api/chat/completed` request.\n            # `request.state` does not persist across these distinct request lifecycles.\n            app_state: State = request.app.state\n            app_state._state[storage_key] = grounding_metadata_obj\n        else:\n            log.debug(f\"Response {message_id} does not have grounding metadata.\")\n\n    @staticmethod\n    def _get_usage_data_event(\n        response: types.GenerateContentResponse,\n    ) -> \"ChatCompletionEvent | None\":\n        \"\"\"\n        Extracts usage data from a GenerateContentResponse object.\n        Returns None if any of the core metrics (prompt_tokens, completion_tokens, total_tokens)\n        cannot be reliably determined.\n\n        Args:\n            response: The GenerateContentResponse object.\n\n        Returns:\n            A dictionary containing the usage data, formatted as a ResponseUsage type,\n            or None if any core metrics are missing.\n        \"\"\"\n\n        if not response.usage_metadata:\n            log.warning(\n                \"Usage_metadata is missing from the response. Cannot reliably determine usage.\"\n            )\n            return None\n\n        usage_data = response.usage_metadata.model_dump()\n        usage_data[\"prompt_tokens\"] = usage_data.pop(\"prompt_token_count\")\n        usage_data[\"completion_tokens\"] = usage_data.pop(\"candidates_token_count\")\n        usage_data[\"total_tokens\"] = usage_data.pop(\"total_token_count\")\n        # Remove null values and turn ModalityTokenCount into dict.\n        for k, v in usage_data.copy().items():\n            if k in (\"prompt_tokens\", \"completion_tokens\", \"total_tokens\"):\n                continue\n            if not v:\n                del usage_data[k]\n\n        completion_event: \"ChatCompletionEvent\" = {\n            \"type\": \"chat:completion\",\n            \"data\": {\"usage\": usage_data},\n        }\n        return completion_event\n\n    # endregion 2.4 Post-processing\n\n    # region 2.5 Logging\n    # TODO: Move to a separate plugin that does not have any Open WebUI funcitonlity and is only imported by this plugin.\n\n    def _is_flat_dict(self, data: Any) -> bool:\n        \"\"\"\n        Checks if a dictionary contains only non-dict/non-list values (is one level deep).\n        \"\"\"\n        if not isinstance(data, dict):\n            return False\n        return not any(isinstance(value, (dict, list)) for value in data.values())\n\n    def _truncate_long_strings(\n        self, data: Any, max_len: int, truncation_marker: str, truncation_enabled: bool\n    ) -> Any:\n        \"\"\"\n        Recursively traverses a data structure (dicts, lists) and truncates\n        long string values. Creates copies to avoid modifying original data.\n\n        Args:\n            data: The data structure (dict, list, str, int, float, bool, None) to process.\n            max_len: The maximum allowed length for string values.\n            truncation_marker: The string to append to truncated values.\n            truncation_enabled: Whether truncation is enabled.\n\n        Returns:\n            A potentially new data structure with long strings truncated.\n        \"\"\"\n        if not truncation_enabled or max_len <= len(truncation_marker):\n            # If truncation is disabled or max_len is too small, return original\n            # Make a copy only if it's a mutable type we might otherwise modify\n            if isinstance(data, (dict, list)):\n                return copy.deepcopy(data)  # Ensure deep copy for nested structures\n            return data  # Primitives are immutable\n\n        if isinstance(data, str):\n            if len(data) > max_len:\n                return data[: max_len - len(truncation_marker)] + truncation_marker\n            return data  # Return original string if not truncated\n        elif isinstance(data, dict):\n            # Process dictionary items, creating a new dict\n            return {\n                k: self._truncate_long_strings(\n                    v, max_len, truncation_marker, truncation_enabled\n                )\n                for k, v in data.items()\n            }\n        elif isinstance(data, list):\n            # Process list items, creating a new list\n            return [\n                self._truncate_long_strings(\n                    item, max_len, truncation_marker, truncation_enabled\n                )\n                for item in data\n            ]\n        else:\n            # Return non-string, non-container types as is (they are immutable)\n            return data\n\n    def plugin_stdout_format(self, record: \"Record\") -> str:\n        \"\"\"\n        Custom format function for the plugin's logs.\n        Serializes and truncates data passed under the 'payload' key in extra.\n        \"\"\"\n\n        # Configuration Keys\n        LOG_OPTIONS_PREFIX = \"_log_\"\n        TRUNCATION_ENABLED_KEY = f\"{LOG_OPTIONS_PREFIX}truncation_enabled\"\n        MAX_LENGTH_KEY = f\"{LOG_OPTIONS_PREFIX}max_length\"\n        TRUNCATION_MARKER_KEY = f\"{LOG_OPTIONS_PREFIX}truncation_marker\"\n        DATA_KEY = \"payload\"\n\n        original_extra = record[\"extra\"]\n        # Extract the data intended for serialization using the chosen key\n        data_to_process = original_extra.get(DATA_KEY)\n\n        serialized_data_json = \"\"\n        if data_to_process is not None:\n            try:\n                serializable_data = pydantic_core.to_jsonable_python(\n                    data_to_process, serialize_unknown=True\n                )\n\n                # Determine truncation settings\n                truncation_enabled = original_extra.get(TRUNCATION_ENABLED_KEY, True)\n                max_length = original_extra.get(MAX_LENGTH_KEY, 256)\n                truncation_marker = original_extra.get(TRUNCATION_MARKER_KEY, \"[...]\")\n\n                # If max_length was explicitly provided, force truncation enabled\n                if MAX_LENGTH_KEY in original_extra:\n                    truncation_enabled = True\n\n                # Truncate long strings\n                truncated_data = self._truncate_long_strings(\n                    serializable_data,\n                    max_length,\n                    truncation_marker,\n                    truncation_enabled,\n                )\n\n                # Serialize the (potentially truncated) data\n                if self._is_flat_dict(truncated_data) and not isinstance(\n                    truncated_data, list\n                ):\n                    json_string = json.dumps(\n                        truncated_data, separators=(\",\", \":\"), default=str\n                    )\n                    # Add a simple prefix if it's compact\n                    serialized_data_json = \" - \" + json_string\n                else:\n                    json_string = json.dumps(truncated_data, indent=2, default=str)\n                    # Prepend with newline for readability\n                    serialized_data_json = \"\\n\" + json_string\n\n            except (TypeError, ValueError) as e:  # Catch specific serialization errors\n                serialized_data_json = f\" - {{Serialization Error: {e}}}\"\n            except (\n                Exception\n            ) as e:  # Catch any other unexpected errors during processing\n                serialized_data_json = f\" - {{Processing Error: {e}}}\"\n\n        # Add the final JSON string (or error message) back into the record\n        record[\"extra\"][\"_plugin_serialized_data\"] = serialized_data_json\n\n        # Base template\n        base_template = (\n            \"<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | \"\n            \"<level>{level: <8}</level> | \"\n            \"<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - \"\n            \"<level>{message}</level>\"\n        )\n\n        # Append the serialized data\n        base_template += \"{extra[_plugin_serialized_data]}\"\n        # Append the exception part\n        base_template += \"\\n{exception}\"\n        # Return the format string template\n        return base_template.rstrip()\n\n    @cache\n    def _add_log_handler(self, log_level: str):\n        \"\"\"\n        Adds or updates the loguru handler specifically for this plugin.\n        Includes logic for serializing and truncating extra data.\n        The handler is added only if the log_level has changed since the last call.\n        \"\"\"\n\n        def plugin_filter(record: \"Record\"):\n            \"\"\"Filter function to only allow logs from this plugin (based on module name).\"\"\"\n            return record[\"name\"] == __name__\n\n        # Get the desired level name and number\n        desired_level_name = log_level\n        try:\n            # Use the public API to get level details\n            desired_level_info = log.level(desired_level_name)\n            desired_level_no = desired_level_info.no\n        except ValueError:\n            log.error(\n                f\"Invalid LOG_LEVEL '{desired_level_name}' configured for plugin {__name__}. Cannot add/update handler.\"\n            )\n            return  # Stop processing if the level is invalid\n\n        # Access the internal state of the log\n        handlers: dict[int, \"Handler\"] = log._core.handlers  # type: ignore\n        handler_id_to_remove = None\n        found_correct_handler = False\n\n        for handler_id, handler in handlers.items():\n            existing_filter = handler._filter  # Access internal attribute\n\n            # Check if the filter matches our plugin_filter\n            # Comparing function objects directly can be fragile if they are recreated.\n            # Comparing by name and module is more robust for functions defined at module level.\n            is_our_filter = (\n                existing_filter is not None  # Make sure a filter is set\n                and hasattr(existing_filter, \"__name__\")\n                and existing_filter.__name__ == plugin_filter.__name__\n                and hasattr(existing_filter, \"__module__\")\n                and existing_filter.__module__ == plugin_filter.__module__\n            )\n\n            if is_our_filter:\n                existing_level_no = handler.levelno\n                log.trace(\n                    f\"Found existing handler {handler_id} for {__name__} with level number {existing_level_no}.\"\n                )\n\n                # Check if the level matches the desired level\n                if existing_level_no == desired_level_no:\n                    log.debug(\n                        f\"Handler {handler_id} for {__name__} already exists with the correct level '{desired_level_name}'.\"\n                    )\n                    found_correct_handler = True\n                    break  # Found the correct handler, no action needed\n                else:\n                    # Found our handler, but the level is wrong. Mark for removal.\n                    log.info(\n                        f\"Handler {handler_id} for {__name__} found, but log level differs \"\n                        f\"(existing: {existing_level_no}, desired: {desired_level_no}). \"\n                        f\"Removing it to update.\"\n                    )\n                    handler_id_to_remove = handler_id\n                    break  # Found the handler to replace, stop searching\n\n        # Remove the old handler if marked for removal\n        if handler_id_to_remove is not None:\n            try:\n                log.remove(handler_id_to_remove)\n                log.debug(f\"Removed handler {handler_id_to_remove} for {__name__}.\")\n            except ValueError:\n                # This might happen if the handler was somehow removed between the check and now\n                log.warning(\n                    f\"Could not remove handler {handler_id_to_remove} for {__name__}. It might have already been removed.\"\n                )\n                # If removal failed but we intended to remove, we should still proceed to add\n                # unless found_correct_handler is somehow True (which it shouldn't be if handler_id_to_remove was set).\n\n        # Add a new handler if no correct one was found OR if we just removed an incorrect one\n        if not found_correct_handler:\n            log.add(\n                sys.stdout,\n                level=desired_level_name,\n                format=self.plugin_stdout_format,\n                filter=plugin_filter,\n            )\n            log.debug(\n                f\"Added new handler to loguru for {__name__} with level {desired_level_name}.\"\n            )\n\n    # endregion 2.5 Logging\n\n    # region 2.6 Utility helpers\n\n    # TODO: Check availability of companion filter too with this method.\n    @staticmethod\n    def _is_function_active(id: str) -> bool:\n        # Get the filter's data from the database.\n        companion_filter = Functions.get_function_by_id(id)\n        # Return if the filter is installed and active.\n        return bool(companion_filter and companion_filter.is_active)\n\n    @staticmethod\n    def _get_merged_valves(\n        default_valves: \"Pipe.Valves\",\n        user_valves: \"Pipe.UserValves | None\",\n        user_email: str,\n    ) -> \"Pipe.Valves\":\n        \"\"\"\n        Merges UserValves into a base Valves configuration.\n\n        The general rule is that if a field in UserValves is not None, it overrides\n        the corresponding field in the default_valves. Otherwise, the default_valves\n        field value is used.\n\n        Exceptions:\n        - If default_valves.USER_MUST_PROVIDE_AUTH_CONFIG is True, then GEMINI_API_KEY and\n          VERTEX_PROJECT in the merged result will be taken directly from\n          user_valves (even if they are None), ignoring the values in default_valves.\n\n        Args:\n            default_valves: The base Valves object with default configurations.\n            user_valves: An optional UserValves object with user-specific overrides.\n                         If None, a copy of default_valves is returned.\n\n        Returns:\n            A new Valves object representing the merged configuration.\n        \"\"\"\n        if user_valves is None:\n            # If no user-specific valves are provided, return a copy of the default valves.\n            return default_valves.model_copy(deep=True)\n\n        # Start with the values from the base `Valves`\n        merged_data = default_valves.model_dump()\n\n        # Override with non-None values from `UserValves`\n        # Iterate over fields defined in the UserValves model\n        for field_name in Pipe.UserValves.model_fields:\n            # getattr is safe as field_name comes from model_fields of user_valves' type\n            user_value = getattr(user_valves, field_name)\n            if user_value is not None and user_value != \"\":\n                # Only update if the field is also part of the main Valves model\n                # (keys of merged_data are fields of default_valves)\n                if field_name in merged_data:\n                    merged_data[field_name] = user_value\n\n        user_whitelist = (\n            default_valves.AUTH_WHITELIST.split(\",\")\n            if default_valves.AUTH_WHITELIST\n            else []\n        )\n\n        # Apply special logic based on default_valves.USER_MUST_PROVIDE_AUTH_CONFIG\n        if (\n            default_valves.USER_MUST_PROVIDE_AUTH_CONFIG\n            and user_email not in user_whitelist\n        ):\n            # If USER_MUST_PROVIDE_AUTH_CONFIG is True and user is not in the whitelist,\n            # then user must provide their own GEMINI_API_KEY\n            # User is disallowed from using Vertex AI in this case.\n            merged_data[\"GEMINI_API_KEY\"] = user_valves.GEMINI_API_KEY\n            merged_data[\"VERTEX_PROJECT\"] = None\n            merged_data[\"USE_VERTEX_AI\"] = False\n\n        # Create a new Valves instance with the merged data.\n        # Pydantic will validate the data against the Valves model definition during instantiation.\n        return Pipe.Valves(**merged_data)\n\n    def _get_first_candidate(\n        self, candidates: list[types.Candidate] | None\n    ) -> types.Candidate | None:\n        \"\"\"Selects the first candidate, logging a warning if multiple exist.\"\"\"\n        if not candidates:\n            # Logging warnings is handled downstream.\n            return None\n        if len(candidates) > 1:\n            log.warning(\"Multiple candidates found, defaulting to first candidate.\")\n        return candidates[0]\n\n    # endregion 2.6 Utility helpers\n\n    # endregion 2. Helper methods inside the Pipe class"}]