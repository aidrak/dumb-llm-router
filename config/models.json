{
  "models": {
    "4.1-nano": {
      "provider": "openai",
      "model_id": "gpt-4.1-nano",
      "type": "chat",
      "system_prompt_file": "prompts/default_prompt.txt",
      "parameters": {
        "max_tokens": 4096
      }
    },
    "4o-mini": {
      "provider": "openai",
      "model_id": "gpt-4o-mini-search-preview",
      "type": "chat",
      "system_prompt_file": "prompts/default_prompt.txt",
      "parameters": {
        "web_search_options": {},
        "exclude_temperature": true,
        "max_tokens": 4096
      }
    },
    "Flash-No-Research": {
      "provider": "gemini",
      "model_id": "gemini-2.5-flash",
      "type": "chat",
      "system_prompt_file": "prompts/default_prompt.txt",
      "parameters": {
        "generation_config": {
          "max_output_tokens": 4096
        }
      }
    },
    "Flash-Research": {
      "provider": "gemini",
      "model_id": "gemini-2.5-flash",
      "type": "chat",
      "system_prompt_file": "prompts/default_prompt.txt",
      "parameters": {
        "enable_google_search": true,
        "generation_config": {
          "max_output_tokens": 4096
        }
      }
    },
    "Gemini-Pro": {
      "provider": "gemini",
      "model_id": "gemini-2.5-pro",
      "type": "chat",
      "system_prompt_file": "prompts/default_prompt.txt",      
      "parameters": {
        "enable_google_search": true,
        "generation_config": {
          "max_output_tokens": 8192,
          "top_p": 0.95
        }
      }
    },
    "Perplexity-Research": {
      "provider": "perplexity",
      "model_id": "sonar-pro",
      "type": "chat",
      "system_prompt_file": "prompts/research_prompt.txt",
      "parameters": {
        "max_tokens": 4096,
        "return_citations": true,
        "search_recency_filter": "month",
        "search_domain_filter": ["*"],
        "web_search_options": {
          "search_context_size": "medium"
        }
      }
    },
    "default": {
      "provider": "gemini",
      "model_id": "gemini-2.5-flash",
      "type": "chat",
      "system_prompt_file": "prompts/default_prompt.txt",
      "parameters": {
        "generation_config": {
          "max_output_tokens": 4096
        }
      }
    }
  }
}
