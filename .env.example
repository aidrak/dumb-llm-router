# Smart LLM Router Environment Variables
# Copy this file to .env and fill in your actual values

# API Keys - Required
OPENAI_API_KEY=your_openai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
PERPLEXITY_API_KEY=your_perplexity_api_key_here

# Ollama Configuration (if using local models)
OLLAMA_API_HOST=http://172.20.20.9:11434

# OpenWebUI Integration (optional)
OPENWEBUI_URL=http://172.20.20.9:3000

# Configuration Paths
CONFIG_PATH=/config
MODEL_CONFIG_FILE=models.json

# Model Routing Configuration (optional - can be set in config.yaml instead)
CLASSIFIER_MODEL=classifier
SIMPLE_NO_RESEARCH_MODEL=4.1-nano
SIMPLE_RESEARCH_MODEL=4o-mini
HARD_NO_RESEARCH_MODEL=Flash-No-Research
HARD_RESEARCH_MODEL=Flash-Research
ESCALATION_MODEL=Gemini-Pro
FALLBACK_MODEL=Flash-No-Research

# Unraid Docker Configuration
APPDATA=/mnt/user/appdata

# Network Configuration (typical Unraid setup)
OLLAMA_API_HOST=http://172.20.20.9:11434
OPENWEBUI_URL=http://172.20.20.9:3000