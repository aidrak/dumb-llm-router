# Smart LLM Router Environment Variables
# Copy this file to .env and fill in your actual values

# API Keys - Required
OPENAI_API_KEY=your_openai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
PERPLEXITY_API_KEY=your_perplexity_api_key_here

# Ollama Configuration (if using local models)
OLLAMA_API_HOST=http://localhost:11434

# OpenWebUI Integration (optional)
OPENWEBUI_URL=http://localhost:3000

# RAG Configuration
# Set to true to use Gemini's native RAG processing instead of Open WebUI's RAG
# When enabled, files are sent directly to Gemini for more efficient processing
# When disabled (default), uses Open WebUI's traditional RAG approach
USE_GEMINI_RAG=false

# Configuration Paths
CONFIG_PATH=/config
MODEL_CONFIG_FILE=models.json

# Unraid Docker Configuration
APPDATA=/mnt/user/appdata